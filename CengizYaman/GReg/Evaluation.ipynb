{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Mount your Google Drive to use datasets (For colab users only)"
      ],
      "metadata": {
        "id": "c43q4jWKTrV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igUKRRV9JGYd",
        "outputId": "1e03e2b9-549b-49c8-c882-0ec2aeb10f8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "JosA9jb1TwMS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hMokIXno90Am"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and Test Datasets"
      ],
      "metadata": {
        "id": "oc9EQi1vT25D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class iSUNDataset(Dataset):\n",
        "    def __init__(self, directory, transform=None):\n",
        "        self.directory = directory\n",
        "        self.image_paths = [os.path.join(directory, fname) for fname in os.listdir(directory) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')  # Ensure images are in RGB mode\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, 0  # Dummy label for OOD dataset\n"
      ],
      "metadata": {
        "id": "yOr5xL7YrMim"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cifar = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),  # ImageNet mean\n",
        "                         (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "71hNpi1-rsYh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "isun_dataset_path = '/path/to/iSUN'\n",
        "isun_dataset = iSUNDataset(isun_dataset_path, transform=transform_cifar)\n",
        "isun_dataloader = DataLoader(isun_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "cifar10_test = datasets.CIFAR10(root='/path/to/cifar10', train=False, download=True, transform=transform_cifar)\n",
        "cifar10_test_dataloader = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "yJKspIVwrlgN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pretrained model"
      ],
      "metadata": {
        "id": "F-NBhSgOUd9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 10) # number of classes if Cifar-10\n",
        "model.load_state_dict(torch.load(\"/path/to/resnet18.pt\"))\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJBL0BGqqGXw",
        "outputId": "05457831-52ea-490c-f55d-09f5fca9b070"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 172MB/s]\n",
            "<ipython-input-10-b59506a181f8>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/resnet18_cifar10.pt\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "XQfarXCcUrnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, id_dataloader, ood_dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on ID and OOD datasets.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained ResNet-18 model\n",
        "    - id_dataloader: DataLoader for in-distribution data (Cifar-10)\n",
        "    - ood_dataloader: DataLoader for out-of-distribution data (iSUN)\n",
        "    - device: Device to perform computation on\n",
        "\n",
        "    Returns:\n",
        "    - metrics: Dictionary with AUROC and FPR95 metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    id_scores = []\n",
        "    ood_scores = []\n",
        "\n",
        "    # Compute ID scores with progress tracking\n",
        "    print(\"Evaluating In-Distribution (ID) Dataset...\")\n",
        "    for id_inputs, _ in tqdm(id_dataloader, desc=\"ID Progress\", leave=False):\n",
        "        id_inputs = id_inputs.to(device)\n",
        "        with torch.no_grad():\n",
        "            id_outputs = model(id_inputs)\n",
        "            id_outputs = id_outputs\n",
        "            id_energy_scores = -torch.logsumexp(id_outputs, dim=1)\n",
        "            id_scores.extend(id_energy_scores.cpu().numpy())\n",
        "\n",
        "    # Compute OOD scores with progress tracking\n",
        "    print(\"Evaluating Out-of-Distribution (OOD) Dataset...\")\n",
        "    for ood_inputs, _ in tqdm(ood_dataloader, desc=\"OOD Progress\", leave=False):\n",
        "        ood_inputs = ood_inputs.to(device)\n",
        "        with torch.no_grad():\n",
        "            ood_outputs = model(ood_inputs)\n",
        "            ood_outputs = ood_outputs\n",
        "            ood_energy_scores = -torch.logsumexp(ood_outputs, dim=1)\n",
        "            ood_scores.extend(ood_energy_scores.cpu().numpy())\n",
        "\n",
        "    # Invert energy scores so that higher scores correspond to ID\n",
        "    id_scores = -np.array(id_scores)\n",
        "    ood_scores = -np.array(ood_scores)\n",
        "\n",
        "    # Concatenate scores and true labels\n",
        "    y_true = np.concatenate([np.ones(len(id_scores)), np.zeros(len(ood_scores))])\n",
        "    y_scores = np.concatenate([id_scores, ood_scores])\n",
        "\n",
        "    # Calculate AUROC\n",
        "    auroc = roc_auc_score(y_true, y_scores)\n",
        "\n",
        "    # Calculate FPR95\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    # Find the FPR where TPR >= 95%\n",
        "    try:\n",
        "        idx = np.where(tpr >= 0.95)[0][0]\n",
        "        fpr95 = fpr[idx]\n",
        "    except IndexError:\n",
        "        fpr95 = 1.0  # If TPR never reaches 95%\n",
        "\n",
        "    metrics = {\n",
        "        \"AUROC\": auroc,\n",
        "        \"FPR95\": fpr95\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "-aJJtXHuHfDo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = evaluate_model(model, cifar10_test_dataloader, isun_dataloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdqiuHvlqbPO",
        "outputId": "25cfb530-1ce4-430b-9e43-b643cb24d186"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating In-Distribution (ID) Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Out-of-Distribution (OOD) Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"AUROC: {metrics['AUROC']:.4f}, FPR95: {metrics['FPR95']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd4760TGsiZF",
        "outputId": "a126a6a8-91c5-4c1f-be48-2320c4a2028f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUROC: 0.6600, FPR95: 0.9472\n"
          ]
        }
      ]
    }
  ]
}