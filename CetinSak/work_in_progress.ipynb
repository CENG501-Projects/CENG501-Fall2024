{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[range(0, 5), range(0, 5), range(0, 5), range(0, 5), range(0, 5), range(0, 5)]\n",
      "15625\n",
      "[(0, 0, 0, 0, 0, 0), (0, 0, 0, 0, 0, 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 2],\n",
       "        [0, 0, 0, 0, 0, 3],\n",
       "        [0, 0, 0, 0, 0, 4]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 5\n",
    "s = 6\n",
    "\n",
    "import torch\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "sub_lists = [range(m) for _ in range(s)]\n",
    "\n",
    "new_tuples = list(product(*sub_lists))  # s=3 ->  [1], [1,2], [1,2,3]\n",
    "print(sub_lists)\n",
    "print(len(new_tuples))\n",
    "\n",
    "print(new_tuples[:2])\n",
    "new_tuples = list(sum(new_tuples, ()))\n",
    "\n",
    "new_tuples[:2*s]\n",
    "\n",
    "new_tuples = torch.tensor(new_tuples)\n",
    "\n",
    "new_tuples = new_tuples.reshape(-1, m, s) \n",
    "\n",
    "# 10 = 2*(4 + 1) 5 patch, 1 info, 4 non-info a,b 00 00 00 00 a0 \n",
    "\n",
    "new_tuples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRHM: Using sparsity type B\n",
      "at tensor level\n",
      "feature shape: torch.Size([1, 24])\n",
      "unique features: tensor([0, 1, 2, 3])\n",
      "feature: tensor([[3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0]])\n",
      "label: tensor([2])\n",
      "\n",
      "feature shape: torch.Size([1, 24])\n",
      "unique features: tensor([0, 1, 2, 3])\n",
      "feature: tensor([[3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0]])\n",
      "label: tensor([2])\n",
      "\n",
      "at tensor level\n",
      "feature shape: torch.Size([1, 24])\n",
      "unique features: tensor([0, 1, 2, 3, 4, 5])\n",
      "feature: tensor([[0, 2, 0, 0, 3, 0, 0, 0, 5, 0, 4, 0, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0]])\n",
      "label: tensor([1])\n",
      "\n",
      "feature shape: torch.Size([1, 24])\n",
      "unique features: tensor([0, 1, 2, 3, 4, 5])\n",
      "feature: tensor([[0, 2, 0, 0, 3, 0, 0, 0, 5, 0, 4, 0, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0]])\n",
      "label: tensor([1])\n",
      "\n",
      "at tensor level\n",
      "feature shape: torch.Size([1, 24])\n",
      "unique features: tensor([0, 1, 2, 3])\n",
      "feature: tensor([[0, 2, 0, 0, 3, 0, 0, 2, 0, 0, 3, 0, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0]])\n",
      "label: tensor([0])\n",
      "\n",
      "feature shape: torch.Size([1, 24])\n",
      "unique features: tensor([0, 1, 2, 3])\n",
      "feature: tensor([[0, 2, 0, 0, 3, 0, 0, 2, 0, 0, 3, 0, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0]])\n",
      "label: tensor([0])\n",
      "\n",
      "at tensor level\n",
      "feature shape: torch.Size([1, 8])\n",
      "unique features: tensor([1, 2, 3, 4])\n",
      "feature: tensor([[3, 4, 2, 1, 3, 2, 3, 2]])\n",
      "label: tensor([2])\n",
      "\n",
      "feature shape: torch.Size([1, 8])\n",
      "unique features: tensor([1, 2, 3, 4])\n",
      "feature: tensor([[3, 4, 2, 1, 3, 2, 3, 2]])\n",
      "label: tensor([2])\n",
      "\n",
      "at tensor level\n",
      "feature shape: torch.Size([1, 8])\n",
      "unique features: tensor([2])\n",
      "feature: tensor([[2, 2, 2, 2, 2, 2, 2, 2]])\n",
      "label: tensor([1])\n",
      "\n",
      "feature shape: torch.Size([1, 8])\n",
      "unique features: tensor([2])\n",
      "feature: tensor([[2, 2, 2, 2, 2, 2, 2, 2]])\n",
      "label: tensor([1])\n",
      "\n",
      "at tensor level\n",
      "feature shape: torch.Size([1, 8])\n",
      "unique features: tensor([1, 2, 3, 4])\n",
      "feature: tensor([[2, 1, 3, 4, 3, 4, 2, 1]])\n",
      "label: tensor([0])\n",
      "\n",
      "feature shape: torch.Size([1, 8])\n",
      "unique features: tensor([1, 2, 3, 4])\n",
      "feature: tensor([[2, 1, 3, 4, 3, 4, 2, 1]])\n",
      "label: tensor([0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.SRHM.reference_code.hierarchical import *\n",
    "from src.SRHM.srhm import *\n",
    "\n",
    "num_layers=3\n",
    "num_classes=3\n",
    "num_features=5\n",
    "seed=0\n",
    "max_dataset_size=10\n",
    "s=2\n",
    "s0=3\n",
    "\n",
    "trainset = SparseRandomHierarchyModel(\n",
    "    num_features=num_features,\n",
    "    m=1,  # features multiplicity\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    s=s, # tuples size\n",
    "    s0=s0,\n",
    "    input_format=\"long\",\n",
    "    whitening=0,\n",
    "    seed=seed,\n",
    "    train=True,\n",
    "    transform=None,\n",
    "    testsize=-1,\n",
    "    max_dataset_size=max_dataset_size\n",
    ")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "# batch, _, _ \n",
    "\n",
    "for elem in list(trainloader)[:10]:\n",
    "    print(\"at tensor level\")\n",
    "    for subelem in elem:\n",
    "        print(\"feature shape:\", elem[0].shape)\n",
    "        print(\"unique features:\", torch.unique(elem[0][0], dim=0))\n",
    "        print(\"feature:\", elem[0])\n",
    "        print(\"label:\", elem[1])\n",
    "        print()\n",
    "\n",
    "trainset = RandomHierarchyModel(\n",
    "    num_features=num_features,\n",
    "    m=1,  # features multiplicity\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    s=s, # tuples size\n",
    "    input_format=\"long\",\n",
    "    whitening=0,\n",
    "    seed=seed,\n",
    "    train=True,\n",
    "    transform=None,\n",
    "    testsize=-1,\n",
    "    max_dataset_size=max_dataset_size\n",
    ")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "# batch, _, _ \n",
    "\n",
    "for elem in list(trainloader)[:10]:\n",
    "    print(\"at tensor level\")\n",
    "    for subelem in elem:\n",
    "        print(\"feature shape:\", elem[0].shape)\n",
    "        print(\"unique features:\", torch.unique(elem[0][0], dim=0))\n",
    "        print(\"feature:\", elem[0])\n",
    "        print(\"label:\", elem[1])\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hierarchy_learning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhierarchy_learning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhierarchical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sample_hierarchical_rules, sample_data_from_paths\n\u001b[1;32m      3\u001b[0m num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      4\u001b[0m num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hierarchy_learning'"
     ]
    }
   ],
   "source": [
    "from hierarchy_learning.datasets.hierarchical import sample_hierarchical_rules, sample_data_from_paths\n",
    "\n",
    "num_layers=3\n",
    "num_classes=2\n",
    "num_features=3\n",
    "seed=0\n",
    "max_dataset_size=20\n",
    "s=4\n",
    "\n",
    "Pmax = m ** ((s ** num_layers - 1) // (s - 1)) * num_classes\n",
    "\n",
    "paths, _ = sample_hierarchical_rules(\n",
    "    num_features, num_layers, m, num_classes, s, seed=seed\n",
    ")\n",
    "\n",
    "max_dataset_size_fix = max_dataset_size\n",
    "if max_dataset_size_fix is None or max_dataset_size_fix > Pmax:\n",
    "    max_dataset_size_fix = Pmax\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "if Pmax < 5e6:  # there is a crossover in computational time of the two sampling methods around this value of Pmax\n",
    "    samples_indices = torch.randperm(Pmax, generator=g)[:max_dataset_size]\n",
    "else:\n",
    "    samples_indices = torch.randint(Pmax, (2 * max_dataset_size,), generator=g)\n",
    "    samples_indices = torch.unique(samples_indices)\n",
    "    perm = torch.randperm(len(samples_indices), generator=g)[:max_dataset_size]\n",
    "    samples_indices = samples_indices[perm]\n",
    "\n",
    "\n",
    "x, targets = sample_data_from_paths(\n",
    "    samples_indices, paths, m, num_classes, num_layers, s, seed=seed, seed_reset_layer=43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n",
      "tensor([[[0, 2, 2, 0],\n",
      "         [1, 2, 1, 0],\n",
      "         [0, 2, 2, 2],\n",
      "         [2, 2, 1, 0],\n",
      "         [0, 1, 0, 0]],\n",
      "\n",
      "        [[2, 2, 1, 1],\n",
      "         [1, 0, 0, 2],\n",
      "         [1, 1, 2, 2],\n",
      "         [1, 1, 0, 1],\n",
      "         [2, 0, 2, 0]]])\n",
      "tensor([[[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[1, 2, 2, 0],\n",
      "         [2, 1, 1, 0],\n",
      "         [2, 2, 0, 0],\n",
      "         [0, 0, 2, 0],\n",
      "         [0, 1, 1, 2]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]],\n",
      "\n",
      "        [[1, 0, 0, 0],\n",
      "         [0, 1, 0, 0],\n",
      "         [1, 0, 2, 2],\n",
      "         [2, 2, 1, 2],\n",
      "         [0, 2, 0, 0]],\n",
      "\n",
      "        [[2, 1, 0, 2],\n",
      "         [2, 1, 1, 2],\n",
      "         [2, 0, 1, 1],\n",
      "         [1, 1, 0, 2],\n",
      "         [1, 2, 0, 1]]])\n",
      "tensor([[[1, 1, 1, 2],\n",
      "         [2, 2, 1, 0],\n",
      "         [0, 0, 1, 2],\n",
      "         [2, 2, 2, 2],\n",
      "         [1, 0, 2, 2]],\n",
      "\n",
      "        [[1, 1, 0, 1],\n",
      "         [1, 2, 0, 2],\n",
      "         [0, 2, 0, 2],\n",
      "         [1, 1, 1, 0],\n",
      "         [0, 2, 2, 0]],\n",
      "\n",
      "        [[1, 1, 2, 0],\n",
      "         [0, 2, 0, 0],\n",
      "         [1, 2, 1, 0],\n",
      "         [2, 1, 0, 2],\n",
      "         [2, 0, 2, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1, 2],\n",
      "         [2, 2, 1, 0],\n",
      "         [0, 0, 1, 2],\n",
      "         [2, 2, 2, 2],\n",
      "         [1, 0, 2, 2]],\n",
      "\n",
      "        [[1, 1, 2, 0],\n",
      "         [0, 2, 0, 0],\n",
      "         [1, 2, 1, 0],\n",
      "         [2, 1, 0, 2],\n",
      "         [2, 0, 2, 0]],\n",
      "\n",
      "        [[1, 1, 0, 1],\n",
      "         [1, 2, 0, 2],\n",
      "         [0, 2, 0, 2],\n",
      "         [1, 1, 1, 0],\n",
      "         [0, 2, 2, 0]]])\n"
     ]
    }
   ],
   "source": [
    "for elem in paths:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features from params : 3\n",
      "Number of features from data   : 3\n",
      "---\n",
      "Number of classes from params:  2\n",
      "Number of classes from data  :  2\n",
      "---\n",
      "[0, 2, 0, 2, 0, 0, 1, 2, 1, 1, 2, 0, 1, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 1, 2, 1, 0, 1, 2, 1, 0, 0, 2, 2, 0, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 2, 0, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 0, 1, 1, 1, 0] 0\n",
      "[2, 2, 1, 0, 1, 1, 1, 0, 0, 2, 2, 0, 2, 0, 2, 0, 1, 0, 2, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0, 2, 0, 2, 1, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0, 2, 1, 1, 0, 1, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2] 1\n",
      "[2, 1, 0, 2, 0, 0, 1, 2, 2, 1, 0, 2, 1, 2, 1, 0, 1, 1, 2, 0, 1, 0, 2, 2, 1, 1, 2, 0, 2, 1, 0, 2, 2, 0, 2, 0, 2, 1, 0, 2, 1, 0, 2, 2, 1, 2, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 0, 1, 0, 2, 2] 0\n",
      "[0, 0, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 1, 1, 1, 2, 2, 2, 1, 0, 1, 2, 0, 2, 0, 2, 0, 2, 0, 0, 1, 2, 1, 2, 1, 0, 1, 2, 0, 2, 0, 2, 2, 0, 0, 0, 1, 2] 1\n",
      "[1, 0, 2, 2, 2, 2, 1, 0, 2, 1, 0, 2, 2, 1, 0, 2, 2, 2, 1, 0, 0, 2, 0, 2, 1, 1, 2, 0, 1, 1, 1, 2, 1, 1, 0, 1, 0, 2, 0, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 0, 2, 0, 1, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 2] 1\n",
      "[2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 1, 2, 1, 0, 2, 2, 2, 2, 0, 0, 1, 2, 0, 2, 0, 0, 1, 1, 2, 0, 0, 2, 2, 0, 1, 1, 2, 0, 0, 2, 0, 0, 1, 2, 1, 0, 1, 2, 0, 2, 2, 1, 0, 2, 1, 0, 2, 2, 2, 2, 2, 2] 1\n",
      "[1, 1, 1, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 1, 0, 2, 1, 1, 2, 0, 0, 0, 1, 2, 1, 1, 2, 0, 0, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 2, 2, 1, 0, 2, 2, 0, 2, 0, 2, 1, 1, 2, 0, 0, 0, 1, 2] 0\n",
      "[2, 2, 1, 0, 0, 0, 1, 2, 1, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2, 1, 0, 2, 1, 1, 1, 2, 1, 1, 1, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 1, 0, 1, 1, 1, 0, 1, 1, 2, 0, 2, 2, 2, 2, 1, 1, 1, 2] 1\n",
      "[1, 0, 2, 2, 2, 1, 0, 2, 0, 2, 0, 2, 1, 1, 1, 0, 0, 2, 0, 2, 1, 2, 1, 0, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 0, 2, 0, 2, 0, 2, 2, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 2, 0, 2, 0, 0, 2, 0, 0, 1, 1, 2, 0] 0\n",
      "[2, 2, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 1, 0, 2, 2, 1, 0, 2, 2, 2, 2, 1, 1, 0, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 1, 2, 0, 2, 1, 1, 0, 1] 1\n",
      "[1, 0, 2, 2, 1, 1, 1, 2, 2, 1, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 1, 2, 2, 0, 2, 0, 1, 1, 1, 0, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 1, 1, 2, 0, 1, 0, 2, 2, 2, 0, 2, 0] 1\n",
      "[1, 1, 1, 0, 1, 2, 1, 0, 2, 2, 1, 0, 0, 0, 1, 2, 1, 1, 1, 2, 1, 0, 2, 2, 0, 2, 0, 2, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 1, 2, 1, 0, 2, 0, 2, 0, 1, 1, 0, 1, 0, 2, 0, 2, 2, 2, 1, 0] 1\n",
      "[2, 2, 2, 2, 1, 1, 0, 1, 1, 2, 1, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 2, 2, 1, 1, 0, 1, 0, 2, 0, 0, 1, 1, 1, 2, 2, 2, 1, 0, 1, 1, 1, 0, 2, 0, 2, 0, 2, 2, 1, 0, 1, 1, 1, 2] 0\n",
      "[0, 2, 0, 0, 1, 2, 0, 2, 0, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 2, 0, 2, 0, 0, 2, 0, 0, 1, 1, 2, 0, 2, 1, 0, 2, 1, 2, 1, 0, 2, 2, 1, 0, 1, 1, 2, 0, 2, 2, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 2, 2, 1, 0] 0\n",
      "[0, 0, 1, 2, 0, 0, 1, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 1, 2, 2, 2, 1, 0, 2, 1, 0, 2, 1, 2, 0, 2, 1, 2, 1, 0, 1, 1, 2, 0, 0, 2, 0, 0, 1, 1, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 0, 2, 0, 0] 1\n",
      "[1, 2, 1, 0, 1, 2, 1, 0, 2, 2, 1, 0, 1, 1, 2, 0, 1, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 2, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 2, 0, 1, 1, 2, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 1, 2, 0, 2, 1, 0, 2, 2, 1, 0, 2] 1\n",
      "[1, 1, 1, 0, 1, 2, 1, 0, 2, 2, 1, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2, 0, 0, 2, 2, 0, 1, 1, 0, 1, 1, 2, 0, 2, 0, 2, 0, 0, 1, 2, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 2, 2, 1, 0, 2, 0, 2, 2, 0] 1\n",
      "[1, 1, 2, 0, 1, 1, 2, 0, 2, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 1, 2, 0, 1, 1, 1, 2, 1, 2, 1, 0, 0, 0, 1, 2, 1, 2, 0, 2, 2, 1, 0, 2, 1, 1, 1, 2, 2, 1, 0, 2, 0, 2, 2, 0, 1, 2, 0, 2, 1, 0, 2, 2] 1\n",
      "[0, 2, 0, 2, 1, 1, 1, 2, 2, 2, 1, 0, 2, 1, 0, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 2, 0, 1, 2, 1, 0, 0, 0, 1, 2, 0, 2, 0, 2, 2, 0, 2, 0, 1, 0, 2, 2, 0, 2, 2, 0, 1, 1, 1, 2, 0, 0, 1, 2, 1, 2, 1, 0] 1\n",
      "[1, 0, 2, 2, 0, 2, 0, 2, 1, 1, 0, 1, 2, 2, 1, 0, 1, 1, 1, 0, 0, 2, 0, 0, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 0, 2, 1, 0, 2, 2, 2, 1, 0, 2, 0, 2, 2, 0, 1, 1, 2, 0, 1, 1, 2, 0] 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features from params :\", num_features)\n",
    "print(\"Number of features from data   :\", torch.unique(x).numel())\n",
    "print(\"---\")\n",
    "print(\"Number of classes from params: \", num_classes)\n",
    "print(\"Number of classes from data  : \", torch.unique(targets).numel())\n",
    "print(\"---\")\n",
    "\n",
    "for xx, y in zip(x, targets):\n",
    "    print(xx.tolist(), y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 5., 4., 6., 5., 2., 2., 6., 6., 6., 5., 3., 5., 3., 2., 6.,\n",
      "         5., 3., 6., 6., 4., 6., 5., 2., 0., 5., 2., 0., 4., 3.],\n",
      "        [8., 3., 4., 3., 2., 6., 6., 6., 3., 8., 6., 6., 5., 2., 2., 2., 4., 5.,\n",
      "         6., 6., 5., 2., 4., 1., 2., 2., 2., 0., 0., 5., 8., 8.],\n",
      "        [7., 2., 0., 5., 7., 2., 5., 3., 4., 6., 3., 8., 4., 6., 7., 2., 6., 6.,\n",
      "         7., 2., 3., 8., 5., 3., 4., 5., 4., 1., 5., 3., 3., 8.],\n",
      "        [0., 5., 4., 5., 5., 3., 6., 6., 6., 6., 2., 2., 2., 6., 4., 5., 8., 3.,\n",
      "         5., 2., 2., 2., 0., 5., 5., 3., 5., 2., 2., 6., 0., 5.],\n",
      "        [3., 8., 8., 3., 7., 2., 7., 2., 8., 3., 2., 2., 4., 6., 4., 5., 4., 1.,\n",
      "         2., 2., 7., 2., 8., 8., 6., 6., 4., 3., 7., 2., 7., 2.],\n",
      "        [8., 8., 2., 2., 2., 6., 5., 3., 8., 8., 0., 5., 2., 0., 4., 6., 2., 6.,\n",
      "         4., 6., 2., 0., 5., 3., 5., 2., 7., 2., 3., 8., 8., 8.],\n",
      "        [4., 5., 2., 6., 2., 2., 7., 2., 4., 6., 0., 5., 4., 6., 2., 0., 7., 2.,\n",
      "         4., 1., 4., 1., 3., 8., 3., 8., 2., 2., 4., 6., 0., 5.],\n",
      "        [8., 3., 0., 5., 5., 3., 7., 2., 2., 6., 2., 6., 7., 2., 4., 5., 4., 5.,\n",
      "         2., 2., 2., 6., 8., 3., 4., 3., 4., 6., 8., 8., 4., 5.],\n",
      "        [3., 8., 7., 2., 2., 2., 4., 3., 2., 2., 5., 3., 4., 5., 8., 8., 4., 3.,\n",
      "         6., 6., 8., 3., 4., 5., 4., 1., 6., 6., 2., 0., 4., 6.],\n",
      "        [8., 3., 4., 5., 4., 1., 4., 5., 2., 6., 2., 2., 6., 6., 8., 3., 8., 3.,\n",
      "         8., 8., 4., 1., 3., 8., 8., 8., 7., 2., 5., 2., 4., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# pairs\n",
    "\n",
    "def pairs_to_num(xi, n):\n",
    "\n",
    "    \"\"\"\n",
    "        Convert one long input with n-features encoding to n^2 pairs encoding.\n",
    "    \"\"\"\n",
    "    ln = len(xi)\n",
    "    xin = torch.zeros(ln // 2)\n",
    "    for ii, xii in enumerate(xi):\n",
    "        xin[ii // 2] += xii * n ** (1 - ii % 2)\n",
    "    return xin\n",
    "\n",
    "def pairing_features(x, n):\n",
    "    \"\"\"\n",
    "        Batch of inputs from n to n^2 encoding.\n",
    "    \"\"\"\n",
    "    xn = torch.zeros(x.shape[0], x.shape[-1] // 2)\n",
    "    for i, xi in enumerate(x.squeeze()):\n",
    "        xn[i] = pairs_to_num(xi, n)\n",
    "    return xn\n",
    "\n",
    "x_pairs = pairing_features(x, num_features)\n",
    "print(x_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 0.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 1., 1., 1.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0.,  ..., 0., 1., 1.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 1.,  ..., 1., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1.,  ..., 0., 1., 0.],\n",
      "         [1., 0., 0.,  ..., 1., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 1., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# binary \n",
    "def dec2bin(x, bits=None):\n",
    "    \"\"\"\n",
    "    Convert decimal number to binary.\n",
    "    :param x: decimal number\n",
    "    :param bits: number of bits to use. If `None`, the minimum possible is used.\n",
    "    :return: x in binary\n",
    "    \"\"\"\n",
    "    if bits is None:\n",
    "        bits = (x.max() + 1).log2().ceil().item()\n",
    "    x = x.int()\n",
    "    mask = 2 ** torch.arange(bits - 1, -1, -1).to(x.device, x.dtype)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).float()\n",
    "\n",
    "x_binary = dec2bin(x)\n",
    "x_binary = x_binary.permute(0, 2, 1)\n",
    "\n",
    "print(x_binary[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3, 1, 3, 1, 1, 2, 3, 2, 2, 3, 1, 2, 3, 1, 3, 1, 3, 3, 1, 3, 1, 3, 1,\n",
      "         2, 3, 2, 1, 2, 3, 2, 1, 1, 3, 3, 1, 2, 3, 2, 1, 3, 1, 3, 1, 2, 2, 3, 1,\n",
      "         2, 3, 1, 3, 1, 1, 2, 3, 1, 3, 1, 1, 2, 2, 2, 1],\n",
      "        [3, 3, 2, 1, 2, 2, 2, 1, 1, 3, 3, 1, 3, 1, 3, 1, 2, 1, 3, 3, 3, 1, 3, 1,\n",
      "         2, 3, 1, 3, 1, 3, 1, 3, 2, 2, 2, 3, 3, 1, 3, 1, 2, 3, 1, 3, 2, 2, 1, 2,\n",
      "         1, 3, 1, 3, 1, 3, 1, 1, 1, 1, 2, 3, 3, 3, 3, 3],\n",
      "        [3, 2, 1, 3, 1, 1, 2, 3, 3, 2, 1, 3, 2, 3, 2, 1, 2, 2, 3, 1, 2, 1, 3, 3,\n",
      "         2, 2, 3, 1, 3, 2, 1, 3, 3, 1, 3, 1, 3, 2, 1, 3, 2, 1, 3, 3, 2, 3, 2, 1,\n",
      "         2, 2, 2, 3, 2, 2, 1, 2, 2, 3, 2, 1, 2, 1, 3, 3],\n",
      "        [1, 1, 2, 3, 2, 2, 2, 3, 2, 3, 2, 1, 3, 1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 3,\n",
      "         1, 3, 3, 1, 2, 2, 2, 3, 3, 3, 2, 1, 2, 3, 1, 3, 1, 3, 1, 3, 1, 1, 2, 3,\n",
      "         2, 3, 2, 1, 2, 3, 1, 3, 1, 3, 3, 1, 1, 1, 2, 3],\n",
      "        [2, 1, 3, 3, 3, 3, 2, 1, 3, 2, 1, 3, 3, 2, 1, 3, 3, 3, 2, 1, 1, 3, 1, 3,\n",
      "         2, 2, 3, 1, 2, 2, 2, 3, 2, 2, 1, 2, 1, 3, 1, 3, 3, 2, 1, 3, 3, 3, 3, 3,\n",
      "         3, 1, 3, 1, 2, 2, 2, 1, 3, 2, 1, 3, 3, 2, 1, 3],\n",
      "        [3, 3, 3, 3, 1, 3, 1, 3, 1, 3, 3, 1, 2, 3, 2, 1, 3, 3, 3, 3, 1, 1, 2, 3,\n",
      "         1, 3, 1, 1, 2, 2, 3, 1, 1, 3, 3, 1, 2, 2, 3, 1, 1, 3, 1, 1, 2, 3, 2, 1,\n",
      "         2, 3, 1, 3, 3, 2, 1, 3, 2, 1, 3, 3, 3, 3, 3, 3],\n",
      "        [2, 2, 2, 3, 1, 3, 3, 1, 1, 3, 1, 3, 3, 2, 1, 3, 2, 2, 3, 1, 1, 1, 2, 3,\n",
      "         2, 2, 3, 1, 1, 3, 1, 1, 3, 2, 1, 3, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 3, 3,\n",
      "         2, 1, 3, 3, 1, 3, 1, 3, 2, 2, 3, 1, 1, 1, 2, 3],\n",
      "        [3, 3, 2, 1, 1, 1, 2, 3, 2, 3, 2, 1, 3, 2, 1, 3, 1, 3, 3, 1, 1, 3, 3, 1,\n",
      "         3, 2, 1, 3, 2, 2, 2, 3, 2, 2, 2, 3, 1, 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 1,\n",
      "         2, 2, 2, 1, 2, 2, 3, 1, 3, 3, 3, 3, 2, 2, 2, 3],\n",
      "        [2, 1, 3, 3, 3, 2, 1, 3, 1, 3, 1, 3, 2, 2, 2, 1, 1, 3, 1, 3, 2, 3, 2, 1,\n",
      "         2, 2, 2, 3, 3, 3, 3, 3, 2, 2, 2, 1, 3, 1, 3, 1, 3, 3, 2, 1, 2, 2, 2, 3,\n",
      "         2, 2, 1, 2, 3, 1, 3, 1, 1, 3, 1, 1, 2, 2, 3, 1],\n",
      "        [3, 3, 2, 1, 2, 2, 2, 3, 2, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 1, 1, 3, 1, 3,\n",
      "         3, 1, 3, 1, 3, 3, 2, 1, 3, 3, 2, 1, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 3, 3,\n",
      "         3, 3, 3, 3, 3, 2, 1, 3, 2, 3, 1, 3, 2, 2, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "# Long output\n",
    "\n",
    "x_long = x.long() + 1\n",
    "print(x_long[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.3333,  0.0000, -1.3333,  ..., -0.6667, -0.6667, -1.3333]],\n",
      "\n",
      "        [[ 0.0000,  0.0000, -0.6667,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.6667, -1.3333,  ..., -1.3333,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6667, -0.6667,  0.0000,  ..., -1.3333,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.3333,  0.0000, -1.3333,  ...,  0.0000, -0.6667, -1.3333]],\n",
      "\n",
      "        [[-0.6667, -1.3333,  0.0000,  ..., -0.6667,  0.0000, -1.3333]]])\n"
     ]
    }
   ],
   "source": [
    "# decimal output\n",
    "x_decimal = ((x[:, None] + 1) / num_features - 1) * 2\n",
    "print(x_decimal[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 1.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [1., 1., 0.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 1.,  ..., 1., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "         [1., 1., 0.,  ..., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 0.,  ..., 0., 0., 1.],\n",
      "         [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "         [1., 1., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# onehot output\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_onehot = F.one_hot(\n",
    "    x.long(),\n",
    "    num_classes=num_features\n",
    ").float()\n",
    "x_onehot = x_onehot.permute(0, 2, 1)\n",
    "print(x_onehot[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(device='cpu', dtype='float32', seed_init=0, seed_net=0, seed_trainset=0, dataset='hier1', ptr=51, pte=12, batch_size=128, scale_batch_size=0, background_noise=0, num_features=8, m=2, s=2, num_layers=2, num_classes=8, input_format='onehot', whitening=0, auto_regression=0, net='fcn', random_features=0, width=64, net_layers=3, filter_size=2, stride=2, batch_norm=0, bias=1, pmask=0.2, loss='cross_entropy', optim='sgd', scheduler='cosineannealing', lr=0.1, momentum=0.9, weight_decay=0.0005, reg_type='l2', epochs=250, zero_loss_epochs=0, zero_loss_threshold=0.01, rescale_epochs=0, alpha=1.0, stability=0, clustering_error=0, locality=0, save_init_net=1, save_best_net=1, save_last_net=1, save_dynamics=0, pickle='test.pkl', output='test.pkl')\n",
      "{'args': Namespace(device='cpu', dtype='float32', seed_init=0, seed_net=0, seed_trainset=0, dataset='hier1', ptr=51, pte=12, batch_size=128, scale_batch_size=0, background_noise=0, num_features=8, m=2, s=2, num_layers=2, num_classes=8, input_format='onehot', whitening=0, auto_regression=0, net='fcn', random_features=0, width=64, net_layers=3, filter_size=2, stride=2, batch_norm=0, bias=1, pmask=0.2, loss='cross_entropy', optim='sgd', scheduler='cosineannealing', lr=0.1, momentum=0.9, weight_decay=0.0005, reg_type='l2', epochs=250, zero_loss_epochs=0, zero_loss_threshold=0.01, rescale_epochs=0, alpha=1.0, stability=0, clustering_error=0, locality=0, save_init_net=1, save_best_net=1, save_last_net=1, save_dynamics=0, pickle='test.pkl', output='test.pkl'),\n",
      " 'avg_epoch_time': 0.009521503448486329,\n",
      " 'best': {'acc': 75.0,\n",
      "          'epoch': 150,\n",
      "          'net': {'beta': tensor([[ 2.1931e+00,  4.5215e+00,  7.1409e-01, -6.0516e-01, -1.6164e+00,\n",
      "         -1.6927e+00, -3.8333e+00,  1.5677e+00],\n",
      "        [-1.0833e-03, -1.7331e+00, -9.6734e-01,  1.9224e+00, -2.4112e+00,\n",
      "          1.2283e+00, -1.0525e+00,  4.4037e+00],\n",
      "        [ 2.4793e+00,  6.9231e-01,  9.9818e-01,  1.8550e+00,  1.2887e+00,\n",
      "          2.3626e+00, -1.0147e+00, -1.1655e+00],\n",
      "        [-1.2488e+00, -2.6856e+00, -1.3900e+00,  1.2654e-01,  3.7574e+00,\n",
      "         -1.2355e-01,  2.3869e+00, -1.6988e+00],\n",
      "        [-3.9718e+00, -3.5895e+00, -6.2675e-01, -1.3490e+00,  2.1293e-02,\n",
      "          1.3533e+00,  2.0243e+00,  2.8997e+00],\n",
      "        [-1.4496e-01,  3.5017e-01, -7.9371e-02,  7.8782e-01,  4.7554e-02,\n",
      "         -1.0995e+00,  1.6806e+00, -1.4440e+00],\n",
      "        [ 1.7218e+00,  2.9077e-01,  1.1671e+00, -2.6083e+00,  1.6707e+00,\n",
      "         -1.4536e+00, -1.8437e+00, -2.1585e+00],\n",
      "        [-2.6342e+00, -1.5867e+00,  3.1516e+00, -1.3290e+00, -2.1291e+00,\n",
      "          5.4946e+00, -3.9667e+00,  1.0592e+00],\n",
      "        [-4.3341e-01,  4.6549e+00,  1.3784e+00, -2.5412e+00, -1.6324e+00,\n",
      "          1.1195e+00,  2.9808e+00, -3.6219e+00],\n",
      "        [ 7.3333e-01, -3.4775e-01,  8.0976e-01,  7.3719e-01,  1.3416e+00,\n",
      "         -1.1359e+00, -2.4704e+00,  3.3090e+00],\n",
      "        [ 6.1478e-01, -5.0824e-01,  1.0970e+00,  2.1351e+00, -6.5852e-01,\n",
      "         -1.4076e+00,  2.4892e-01,  4.7918e-02],\n",
      "        [-7.5142e-02, -2.9520e-01, -2.6277e-01,  7.9124e-01,  7.0196e-01,\n",
      "          8.7862e-01, -9.4497e-01,  2.6178e-01],\n",
      "        [ 1.3044e+00, -2.9288e+00, -1.6612e+00, -3.1750e+00,  1.1078e+00,\n",
      "         -2.6320e+00, -3.8662e-01, -6.2312e-01],\n",
      "        [ 3.0561e-01,  4.4280e-01, -5.3712e-01, -2.4252e+00,  1.5679e+00,\n",
      "          7.6022e-01,  3.2664e-01,  1.9757e+00],\n",
      "        [ 1.7556e+00, -2.8276e+00, -3.3356e+00, -4.9011e-01, -3.7811e-01,\n",
      "          1.2707e+00,  1.8238e+00,  4.0472e+00],\n",
      "        [ 5.5880e-01,  2.1240e+00, -9.2076e-01,  9.6983e-01, -9.1703e-01,\n",
      "         -6.0337e-01, -1.0137e+00, -2.5425e+00],\n",
      "        [ 2.4861e-01, -1.7545e+00, -7.7286e-01, -9.0517e-01, -1.8629e+00,\n",
      "          1.5608e+00,  1.7061e+00,  2.3468e+00],\n",
      "        [ 7.5725e+00,  2.4378e+00, -4.7578e+00,  2.4695e+00, -4.2493e-01,\n",
      "         -8.4651e+00, -3.4398e+00,  3.8696e+00],\n",
      "        [-4.1491e+00, -4.6500e+00,  3.2535e+00,  1.2788e+00, -1.4277e+00,\n",
      "          3.2900e+00, -9.7812e-01,  7.2634e-01],\n",
      "        [-9.9113e-01, -1.2911e+00,  2.6418e-01, -6.4587e-01, -2.4412e+00,\n",
      "         -4.2181e-01,  2.1967e-01,  4.3605e-02],\n",
      "        [-4.3615e+00, -5.2962e+00,  4.1371e-01,  1.2700e+00,  8.4002e-01,\n",
      "          6.4671e+00,  4.6792e+00, -5.2687e-01],\n",
      "        [-2.3398e+00,  1.3313e+00,  1.9050e+00,  8.2993e-01,  3.2219e+00,\n",
      "          1.7636e+00,  2.0641e+00, -5.3308e+00],\n",
      "        [ 1.4261e-01, -1.4268e+00, -1.7798e+00, -1.7523e+00,  7.2173e-01,\n",
      "         -1.1140e+00,  5.7182e-01,  1.3797e-01],\n",
      "        [-2.7368e+00,  2.8730e+00, -1.6429e+00,  2.4543e+00, -1.3502e+00,\n",
      "         -4.5592e-01, -1.3402e+00, -5.5357e-01],\n",
      "        [-9.3510e-01,  9.9201e-01, -1.1875e+00, -5.6225e-02, -1.4273e+00,\n",
      "          2.0317e+00, -1.2415e+00,  9.6678e-01],\n",
      "        [-3.8188e+00, -2.2275e+00, -2.3417e+00,  2.2304e+00,  1.1486e+00,\n",
      "          1.8198e+00,  1.6084e+00,  1.4715e+00],\n",
      "        [-3.6694e-01, -3.0881e-01, -2.4020e+00, -3.2487e-01, -9.0786e-01,\n",
      "          9.3831e-01, -7.9552e-01, -5.3130e-01],\n",
      "        [ 1.3117e+00,  1.2362e+00, -9.0575e-01, -8.5976e-01, -6.5692e-01,\n",
      "         -6.8438e-01,  3.3302e-01, -6.9217e-01],\n",
      "        [-1.1625e-01, -4.1347e-01,  1.3104e+00,  7.8757e-01, -7.6040e-01,\n",
      "         -3.3547e-01,  6.5741e-02,  1.3543e+00],\n",
      "        [ 2.5582e+00, -2.6057e+00, -2.2288e+00, -1.9002e+00, -2.4870e-01,\n",
      "         -2.4217e+00,  5.9353e-01,  1.0257e+00],\n",
      "        [ 4.3476e+00, -8.1848e-01, -2.5777e+00,  4.4273e-01, -4.2819e+00,\n",
      "         -2.7511e+00, -3.2689e+00,  5.4941e+00],\n",
      "        [ 3.5296e+00,  2.9985e+00, -3.2289e+00,  7.8965e-01, -5.1436e-01,\n",
      "         -2.9906e+00, -1.4834e+00, -3.7146e+00],\n",
      "        [ 1.5217e-01,  2.1728e-01, -1.1616e+00,  1.2197e-01,  7.6603e-01,\n",
      "         -1.1519e+00, -5.7203e-01, -1.3929e+00],\n",
      "        [ 1.6798e+00, -8.5243e+00, -3.5856e+00,  5.7509e-01, -1.3533e+00,\n",
      "          4.1472e+00,  4.1123e+00,  4.5707e+00],\n",
      "        [ 5.7594e-01, -2.9885e+00, -1.2991e+00,  1.8471e+00, -1.5254e+00,\n",
      "         -2.5220e+00,  2.2722e+00,  3.6285e+00],\n",
      "        [-1.6494e+00,  1.3185e+00,  4.0353e+00, -2.7008e+00, -4.2825e-01,\n",
      "          3.0261e+00, -1.4754e+00, -3.0906e+00],\n",
      "        [-3.8253e-01,  6.8803e-01,  1.5992e-01, -3.3746e-01, -9.9232e-02,\n",
      "         -1.0833e+00, -9.0138e-01,  2.3275e-01],\n",
      "        [ 1.2552e-01, -2.6012e+00, -3.0058e+00,  3.1127e-01,  1.3571e-03,\n",
      "          3.1058e+00,  2.8122e+00, -5.4810e-02],\n",
      "        [-7.1287e-01,  2.6019e-01,  1.0107e+00, -9.1694e-02, -2.6557e-01,\n",
      "         -1.4162e+00, -1.6100e+00, -1.8851e+00],\n",
      "        [ 2.3835e-01,  3.3721e+00, -1.3236e+00,  3.2298e+00,  1.0539e-01,\n",
      "         -1.8110e+00,  2.5621e+00, -8.1105e-01],\n",
      "        [ 3.0660e+00,  1.2780e+00, -9.7877e-01, -8.1891e-01,  9.1414e-01,\n",
      "         -1.0504e+00, -2.2001e+00, -1.9426e+00],\n",
      "        [-1.1364e+00, -1.0492e+00, -1.4783e+00,  2.1100e+00, -5.0267e-01,\n",
      "         -1.7177e+00,  1.8315e+00,  7.1898e-01],\n",
      "        [ 1.4924e-01,  3.6357e+00,  1.6903e-01,  1.1119e+00,  5.5246e-02,\n",
      "         -1.9555e+00, -1.3739e+00, -3.3803e-01],\n",
      "        [ 1.0675e+00, -2.9970e-01, -1.9308e+00,  1.0304e+00, -7.9724e-01,\n",
      "         -9.8571e-01,  2.2442e-01,  9.2891e-01],\n",
      "        [ 7.2039e-01, -8.1731e-02,  2.8499e+00,  1.8189e+00,  4.8546e-01,\n",
      "          1.4890e-01,  5.0659e-01,  3.3955e-01],\n",
      "        [ 5.4491e-01,  9.7760e-01,  5.2009e+00,  1.6156e+00,  2.2399e+00,\n",
      "         -2.0303e+00, -3.7008e+00, -5.7829e+00],\n",
      "        [-1.7767e-02,  1.2650e+00, -1.2759e+00,  6.4147e-01,  1.5072e-01,\n",
      "         -9.4027e-01, -1.1425e+00,  5.4036e-01],\n",
      "        [-1.2872e+00,  5.5630e+00,  2.6313e+00,  8.9941e-01,  4.6608e+00,\n",
      "         -2.5178e+00, -3.2345e+00, -4.2925e+00],\n",
      "        [-1.4289e+00,  8.4905e-01,  6.5599e-01, -1.0845e+00,  6.3445e-01,\n",
      "         -2.3543e-01,  9.6976e-01,  8.0507e-01],\n",
      "        [-4.9022e-01,  9.0040e-01, -1.1384e+00,  1.1709e+00,  4.6887e-02,\n",
      "         -2.3579e-01,  1.3466e+00, -1.6744e+00],\n",
      "        [ 3.3548e-01,  1.1538e+00, -2.8040e-01,  6.8724e-01,  1.1791e-03,\n",
      "         -4.2631e-01, -8.4678e-01, -1.2149e-01],\n",
      "        [ 1.2771e+00, -2.1556e+00, -1.9193e+00, -1.4003e+00, -1.3391e+00,\n",
      "          8.6586e-01,  5.5486e+00, -9.8779e-01],\n",
      "        [-3.2646e+00,  4.5580e-01,  2.6943e+00,  4.7438e-01,  1.4733e+00,\n",
      "          3.8979e+00, -6.3134e-01, -4.9734e+00],\n",
      "        [-1.3369e+00,  2.5855e-01,  1.9618e-01,  4.5938e-01,  7.0947e-01,\n",
      "          1.4121e+00,  1.3938e-01,  1.2438e+00],\n",
      "        [ 5.0777e+00, -2.7151e+00, -2.2473e+00, -3.0034e+00, -2.1422e+00,\n",
      "         -1.3690e-01,  2.0423e+00, -1.2262e-01],\n",
      "        [-2.1409e+00, -1.2441e+00,  1.1062e+00,  3.2582e-01,  2.9841e+00,\n",
      "         -6.7464e-01,  1.4628e+00, -1.1297e+00],\n",
      "        [-2.6538e+00,  2.4743e+00,  5.2095e+00,  1.6274e+00,  1.8735e+00,\n",
      "         -4.2716e-03, -4.3439e+00, -2.8734e+00],\n",
      "        [ 1.5523e+00, -5.7267e-01, -2.4401e+00,  1.2873e-01, -3.6647e-01,\n",
      "         -9.3389e-01,  2.7766e+00, -1.3047e+00],\n",
      "        [ 2.9182e-01,  1.9377e+00,  1.9417e+00, -2.5306e+00, -1.7568e+00,\n",
      "          4.2979e-01, -1.6392e-01, -1.3187e+00],\n",
      "        [-1.7212e+00, -1.0866e+00,  3.9257e+00,  6.2678e-02,  3.1481e+00,\n",
      "          1.1728e+00,  8.2242e-02, -2.1162e+00],\n",
      "        [ 1.9635e+00,  3.8709e+00, -2.0768e+00,  3.1262e+00, -2.1635e+00,\n",
      "         -2.6441e+00, -1.4079e+00, -1.0426e+00],\n",
      "        [ 4.3763e+00,  9.8441e-01, -3.2433e+00, -1.9023e+00, -1.5354e+00,\n",
      "         -2.4000e+00,  2.3090e+00,  1.6294e+00],\n",
      "        [ 2.7624e+00, -1.2139e+00, -2.5205e-01, -6.2698e-01,  1.6986e+00,\n",
      "         -8.9997e-01,  8.0598e-01,  2.3221e+00],\n",
      "        [-4.8141e+00,  2.3717e+00,  1.8586e-01, -1.2660e+00,  5.1948e+00,\n",
      "         -1.2430e+00,  3.0947e+00, -4.5612e+00]]),\n",
      "                  'hier.0.bias': tensor([[ 1.1371,  1.7072,  1.7408,  0.4715, -0.3463, -0.4742, -0.6916,  0.0942,\n",
      "          0.5932,  0.2439,  0.6867, -0.7963,  0.4131, -0.0485,  2.6193, -0.7089,\n",
      "          0.9622,  3.0121,  0.4456,  1.2212,  1.2681,  1.1726, -0.1560,  0.5365,\n",
      "          1.6599, -2.4783,  0.1257,  2.6166,  0.6188,  1.1094, -0.6714,  2.0099,\n",
      "          0.1863,  0.7276,  0.5835,  2.6493,  1.3704,  1.5854, -0.2988,  0.1539,\n",
      "          2.7421,  1.6617,  0.2808,  3.1561, -0.4171,  1.7701,  1.9499,  2.1240,\n",
      "          1.2991,  1.9579, -0.3056, -0.7326,  2.1232,  1.3481,  0.3968,  0.5142,\n",
      "         -0.4288, -0.3180, -0.1849,  1.9325,  1.0687, -0.3487, -0.5082,  1.4347]]),\n",
      "                  'hier.0.weight': tensor([[-1.3453, -1.1492,  0.1397,  ...,  1.6554,  0.6763, -1.0367],\n",
      "        [-0.2169, -0.2368, -0.3346,  ..., -1.2318,  2.3978,  0.0331],\n",
      "        [-1.7570,  1.0186,  2.5766,  ..., -1.0454,  0.6781,  1.3528],\n",
      "        ...,\n",
      "        [ 3.1062, -0.4116,  1.3793,  ...,  0.1761, -0.7047, -0.7367],\n",
      "        [ 0.3402, -0.9261,  0.1372,  ..., -0.6480, -0.8635, -0.6948],\n",
      "        [-0.7378, -2.0606,  0.2858,  ..., -0.2948, -1.3724,  1.4015]]),\n",
      "                  'hier.2.0.bias': tensor([[ 1.6861, -0.3580, -0.3154,  1.5124,  0.4603,  1.5646,  0.1360,  0.0533,\n",
      "          1.7507,  1.4341,  1.0630, -0.9133,  0.5709,  0.3247, -0.3972,  0.2233,\n",
      "          0.3983,  0.1082,  0.0038, -0.2248,  0.7442,  1.3439,  1.2720,  1.7050,\n",
      "          1.2338, -0.7176,  0.2541,  0.1338,  2.3904, -0.4574,  0.2923,  1.0426,\n",
      "          0.0116,  1.9761,  1.4398, -1.3319, -0.8726, -0.8022, -0.2035,  1.4208,\n",
      "          2.8293,  0.0901, -2.0284,  0.9074, -0.3504,  1.1698,  1.0076,  0.7070,\n",
      "          0.2643,  0.7263,  1.9237,  1.9474,  0.1109,  0.3379,  0.4131, -1.7943,\n",
      "          1.4548,  0.4277, -0.8147,  0.2613, -2.3221, -0.0663,  0.3317, -0.1375]]),\n",
      "                  'hier.2.0.weight': tensor([[-1.2236, -0.6363,  1.3602,  ..., -0.2871,  0.4078, -1.0883],\n",
      "        [-0.0031,  0.6624, -0.5056,  ..., -0.9544, -0.0722, -0.9598],\n",
      "        [-1.0257,  0.2247, -0.3579,  ...,  0.4095,  1.3627, -1.0141],\n",
      "        ...,\n",
      "        [ 0.5530, -0.2603,  1.5146,  ..., -2.0597,  1.6787, -1.1524],\n",
      "        [-0.6875, -1.8031, -0.6386,  ...,  0.6031, -0.2872,  0.0912],\n",
      "        [ 1.0390,  0.8400,  1.3756,  ..., -0.3969,  0.3246,  0.7052]]),\n",
      "                  'hier.3.0.bias': tensor([[ 1.4972e-02,  5.1798e-01, -8.4541e-01,  3.6304e-03,  4.7296e-01,\n",
      "         -8.6370e-01, -8.1321e-01,  1.0213e+00,  1.8013e+00,  6.4248e-01,\n",
      "         -1.9029e+00, -1.0091e+00,  1.4191e-01,  3.5616e-01, -5.9653e-01,\n",
      "         -1.6523e-01,  1.4171e+00,  2.0736e+00,  1.4587e+00,  4.2638e-01,\n",
      "          8.5834e-01,  1.1202e+00,  9.8729e-02,  6.7574e-01, -8.3868e-01,\n",
      "          2.4302e+00, -1.1161e+00, -9.0569e-01, -6.5433e-01, -3.3450e-01,\n",
      "          1.6684e+00, -2.8120e-01, -1.0260e-01,  4.1590e-01,  1.7659e-02,\n",
      "         -5.5601e-01, -9.2779e-01, -6.5239e-02,  9.7773e-02,  1.0986e+00,\n",
      "         -4.0522e-04, -9.1886e-02,  1.0213e+00, -1.7857e+00, -4.9751e-01,\n",
      "          1.0583e+00, -3.4054e-01,  1.6824e+00, -4.5283e-01,  5.8408e-01,\n",
      "         -5.2977e-01,  6.6996e-02, -4.1389e-01, -1.9373e-01,  1.0948e+00,\n",
      "          1.9601e+00, -1.5962e-01,  1.5553e+00,  9.4971e-01,  1.6715e+00,\n",
      "          1.0260e+00,  1.7391e+00, -2.3222e+00,  1.6001e+00]]),\n",
      "                  'hier.3.0.weight': tensor([[-1.7857, -0.8052,  0.0183,  ..., -0.2358, -1.7892, -0.7551],\n",
      "        [-1.4570, -0.5787, -3.0837,  ..., -0.7276,  0.4480, -0.6238],\n",
      "        [ 1.5302,  0.0284, -0.9547,  ...,  0.1344, -2.3593,  0.5207],\n",
      "        ...,\n",
      "        [-0.5233,  0.7523, -0.6380,  ...,  0.0079, -0.2818,  0.7326],\n",
      "        [-1.2753, -0.1229,  0.2428,  ..., -0.7041,  0.4481, -0.8174],\n",
      "        [ 1.9709,  0.1736,  0.9880,  ..., -0.8047,  1.1234,  0.6891]])}},\n",
      " 'clustering_error': [],\n",
      " 'dynamics': None,\n",
      " 'epoch': [1,\n",
      "           2,\n",
      "           3,\n",
      "           4,\n",
      "           5,\n",
      "           6,\n",
      "           7,\n",
      "           8,\n",
      "           9,\n",
      "           10,\n",
      "           11,\n",
      "           12,\n",
      "           13,\n",
      "           14,\n",
      "           15,\n",
      "           16,\n",
      "           17,\n",
      "           18,\n",
      "           19,\n",
      "           20,\n",
      "           21,\n",
      "           22,\n",
      "           23,\n",
      "           24,\n",
      "           25,\n",
      "           26,\n",
      "           27,\n",
      "           28,\n",
      "           29,\n",
      "           30,\n",
      "           31,\n",
      "           32,\n",
      "           33,\n",
      "           34,\n",
      "           35,\n",
      "           36,\n",
      "           37,\n",
      "           38,\n",
      "           39,\n",
      "           40,\n",
      "           41,\n",
      "           42,\n",
      "           43,\n",
      "           44,\n",
      "           45,\n",
      "           46,\n",
      "           47,\n",
      "           48,\n",
      "           49,\n",
      "           50,\n",
      "           51,\n",
      "           52,\n",
      "           53,\n",
      "           54,\n",
      "           55,\n",
      "           56,\n",
      "           57,\n",
      "           58,\n",
      "           59,\n",
      "           60,\n",
      "           61,\n",
      "           62,\n",
      "           63,\n",
      "           64,\n",
      "           65,\n",
      "           66,\n",
      "           67,\n",
      "           68,\n",
      "           69,\n",
      "           70,\n",
      "           71,\n",
      "           72,\n",
      "           73,\n",
      "           74,\n",
      "           75,\n",
      "           76,\n",
      "           77,\n",
      "           78,\n",
      "           79,\n",
      "           80,\n",
      "           81,\n",
      "           82,\n",
      "           83,\n",
      "           84,\n",
      "           85,\n",
      "           86,\n",
      "           87,\n",
      "           88,\n",
      "           89,\n",
      "           90,\n",
      "           91,\n",
      "           92,\n",
      "           93,\n",
      "           94,\n",
      "           95,\n",
      "           96,\n",
      "           97,\n",
      "           98,\n",
      "           99,\n",
      "           100,\n",
      "           101,\n",
      "           102,\n",
      "           103,\n",
      "           104,\n",
      "           105,\n",
      "           106,\n",
      "           107,\n",
      "           108,\n",
      "           109,\n",
      "           110,\n",
      "           111,\n",
      "           112,\n",
      "           113,\n",
      "           114,\n",
      "           115,\n",
      "           116,\n",
      "           117,\n",
      "           118,\n",
      "           119,\n",
      "           120,\n",
      "           121,\n",
      "           122,\n",
      "           123,\n",
      "           124,\n",
      "           125,\n",
      "           126,\n",
      "           127,\n",
      "           128,\n",
      "           129,\n",
      "           130,\n",
      "           131,\n",
      "           132,\n",
      "           133,\n",
      "           134,\n",
      "           135,\n",
      "           136,\n",
      "           137,\n",
      "           138,\n",
      "           139,\n",
      "           140,\n",
      "           141,\n",
      "           142,\n",
      "           143,\n",
      "           144,\n",
      "           145,\n",
      "           146,\n",
      "           147,\n",
      "           148,\n",
      "           149,\n",
      "           150,\n",
      "           151,\n",
      "           152,\n",
      "           153,\n",
      "           154,\n",
      "           155,\n",
      "           156,\n",
      "           157,\n",
      "           158,\n",
      "           159,\n",
      "           160,\n",
      "           161,\n",
      "           162,\n",
      "           163,\n",
      "           164,\n",
      "           165,\n",
      "           166,\n",
      "           167,\n",
      "           168,\n",
      "           169,\n",
      "           170,\n",
      "           171,\n",
      "           172,\n",
      "           173,\n",
      "           174,\n",
      "           175,\n",
      "           176,\n",
      "           177,\n",
      "           178,\n",
      "           179,\n",
      "           180,\n",
      "           181,\n",
      "           182,\n",
      "           183,\n",
      "           184,\n",
      "           185,\n",
      "           186,\n",
      "           187,\n",
      "           188,\n",
      "           189,\n",
      "           190,\n",
      "           191,\n",
      "           192,\n",
      "           193,\n",
      "           194,\n",
      "           195,\n",
      "           196,\n",
      "           197,\n",
      "           198,\n",
      "           199,\n",
      "           200,\n",
      "           201,\n",
      "           202,\n",
      "           203,\n",
      "           204,\n",
      "           205,\n",
      "           206,\n",
      "           207,\n",
      "           208,\n",
      "           209,\n",
      "           210,\n",
      "           211,\n",
      "           212,\n",
      "           213,\n",
      "           214,\n",
      "           215,\n",
      "           216,\n",
      "           217,\n",
      "           218,\n",
      "           219,\n",
      "           220,\n",
      "           221,\n",
      "           222,\n",
      "           223,\n",
      "           224,\n",
      "           225,\n",
      "           226,\n",
      "           227,\n",
      "           228,\n",
      "           229,\n",
      "           230,\n",
      "           231,\n",
      "           232,\n",
      "           233,\n",
      "           234,\n",
      "           235,\n",
      "           236,\n",
      "           237,\n",
      "           238,\n",
      "           239,\n",
      "           240,\n",
      "           241,\n",
      "           242,\n",
      "           243,\n",
      "           244,\n",
      "           245,\n",
      "           246,\n",
      "           247,\n",
      "           248,\n",
      "           249,\n",
      "           250],\n",
      " 'init': {'beta': tensor([[ 0.6902,  1.9876,  0.4430, -1.0210, -1.5080,  0.0260, -0.5814,  1.2264],\n",
      "        [ 0.9996, -0.0134, -0.1486, -0.1446, -0.8374,  0.8868, -0.5224,  1.1847],\n",
      "        [ 0.9185,  0.7911,  1.1975,  1.9890,  1.0059,  2.6553, -0.6122, -0.3653],\n",
      "        [ 0.2821, -1.9935, -0.8015, -0.0964,  1.5911, -0.7389,  1.2630, -0.3914],\n",
      "        [-1.9302, -0.3421, -0.1848, -1.7143,  0.2630, -1.2148,  0.4759,  1.3727],\n",
      "        [-0.1466,  0.3541, -0.0803,  0.7966,  0.0481, -1.1118,  1.6994, -1.4602],\n",
      "        [ 0.7736, -0.9257,  0.4725, -2.3007,  0.7182, -0.7501, -0.5852, -0.6525],\n",
      "        [-0.1339,  0.4161, -0.1212, -1.0830, -1.6144,  2.2343, -1.6721,  0.0123],\n",
      "        [-0.4048,  2.0595,  0.5953, -0.6005, -1.8891,  0.1665,  1.2074,  0.7916],\n",
      "        [ 0.2213,  0.3530,  0.4033,  0.0941,  1.0285, -0.2378, -0.8809,  2.0286],\n",
      "        [ 0.9548, -0.3707,  1.3488,  1.8399, -0.5425, -0.8151, -0.5689, -0.2593],\n",
      "        [-0.0760, -0.2985, -0.2657,  0.8001,  0.7098,  0.8885, -0.9556,  0.2647],\n",
      "        [-0.3820, -2.1060, -2.0434, -2.5008,  0.5071, -1.6395, -0.0732, -0.8575],\n",
      "        [ 0.5758,  0.6075, -0.5345, -2.3495,  1.7228,  0.9183,  0.6054,  0.8979],\n",
      "        [ 0.3993,  0.4344, -0.5581, -0.5090,  0.8502,  0.3047, -0.0143,  0.9794],\n",
      "        [ 0.4824, -0.5975, -0.1233,  0.2520, -0.7320,  0.2508, -0.2165, -1.6868],\n",
      "        [ 0.2580, -0.0458, -0.4626, -0.9859,  0.1197, -0.4387,  0.8346,  1.2939],\n",
      "        [ 2.1250,  0.2555, -1.1267, -0.4185, -0.5118, -1.8685, -0.6045,  1.4030],\n",
      "        [-0.9821, -1.4652,  0.7410,  0.2228, -1.9297, -0.1998,  0.5413,  0.3858],\n",
      "        [-0.9664, -1.3981,  0.2995, -0.6048, -2.4351, -0.3600,  0.0360,  0.1063],\n",
      "        [-0.3968,  0.1606, -0.2039,  1.4760,  0.3693,  0.8492,  1.0742,  0.1958],\n",
      "        [-0.1705,  0.8203,  0.2705,  0.7876,  0.9977, -0.0174,  2.0728, -1.2771],\n",
      "        [ 0.1582, -1.4171, -1.9311, -1.7452,  0.7565, -1.1266,  0.6062,  0.1499],\n",
      "        [-1.9199,  1.2230, -1.0360,  1.4985, -1.5337, -0.1128, -0.7499, -0.1523],\n",
      "        [-1.0392,  1.0314, -1.1962,  0.0136, -1.5282,  2.0866, -1.2860,  1.0513],\n",
      "        [-1.0231, -0.4073, -1.0950,  0.6777,  1.2168,  0.1128,  0.1084,  0.2994],\n",
      "        [-0.3711, -0.3123, -2.4289, -0.3285, -0.9180,  0.9488, -0.8044, -0.5373],\n",
      "        [ 1.3264,  1.2500, -0.9159, -0.8694, -0.6643, -0.6920,  0.3367, -0.6999],\n",
      "        [ 0.3055, -0.2012,  1.5954,  0.6752, -0.5500, -0.1332, -0.0324,  0.2542],\n",
      "        [-0.2945, -1.8219, -1.5372, -1.3768,  0.2997, -1.1382,  0.4145,  0.1682],\n",
      "        [-0.1223, -0.2678, -0.6965, -0.3098, -2.4048,  0.4118, -1.3211,  1.2585],\n",
      "        [ 1.0392,  0.6215, -2.5066,  0.3164, -1.0105, -0.9509, -0.2013, -1.9735],\n",
      "        [ 0.1551,  0.2200, -1.1823,  0.1246,  0.7758, -1.1636, -0.5772, -1.4073],\n",
      "        [ 0.7972, -1.5432, -0.4384,  0.4793, -0.7568,  1.0116,  0.3062,  1.7841],\n",
      "        [ 0.0808, -1.7537, -0.2137,  1.2925, -0.5698, -1.7117,  1.4407,  1.4236],\n",
      "        [ 0.0434,  0.3126,  0.7702, -1.2431, -1.5338, -0.1342,  0.4400,  0.3695],\n",
      "        [-0.3856,  0.6970,  0.1629, -0.3400, -0.0991, -1.0949, -0.9194,  0.2366],\n",
      "        [ 0.1465, -1.8182, -1.8236,  0.0423,  0.3667,  1.9397,  1.2545,  0.5941],\n",
      "        [-0.6042, -0.2437,  1.3137, -0.3359, -0.3408, -1.3252, -1.5161, -1.7111],\n",
      "        [ 0.0836,  1.9464, -0.1532,  1.4384,  0.4216, -0.2008,  1.8755,  0.2130],\n",
      "        [ 0.6535,  0.3124, -0.6394, -0.4871, -0.1133, -0.1769, -1.1649, -0.1364],\n",
      "        [-0.7325, -0.5977, -0.9986,  1.4869, -0.1051, -1.4881,  0.5854,  0.6122],\n",
      "        [ 0.5095, -0.2678, -0.0934, -0.7123,  0.0970,  0.7696,  0.7010,  0.4663],\n",
      "        [ 1.0795, -0.3030, -1.9524,  1.0420, -0.8059, -0.9971,  0.2269,  0.9393],\n",
      "        [ 1.0179, -0.3418,  1.1637,  2.1078,  0.6602,  0.3446,  1.2315,  0.6800],\n",
      "        [ 0.7361, -0.6431,  1.4848,  1.0492, -0.2422, -1.2365,  0.0467, -2.1404],\n",
      "        [-0.0114,  1.2861, -1.2888,  0.5917,  0.1656, -0.9611, -1.1288,  0.5592],\n",
      "        [-0.5513,  1.8650, -0.2826,  0.6940,  1.2468, -0.3623, -0.3274,  0.1674],\n",
      "        [-1.4120,  0.8933,  0.6069, -1.1226,  0.6821, -0.1909,  0.8737,  0.8482],\n",
      "        [-0.1629,  1.1675, -0.5079,  0.6079,  0.2674, -0.3231,  0.5581, -1.6820],\n",
      "        [ 0.8468,  1.3855, -0.1581, -0.2572,  0.1646, -0.6897, -0.4661, -0.3174],\n",
      "        [ 1.0579,  0.3971,  0.3160, -1.3905, -1.0879, -0.7200,  1.3550, -0.0392],\n",
      "        [-0.3603,  0.7518,  0.0173,  0.8482, -0.6143,  1.1261, -1.4706, -0.1703],\n",
      "        [-1.5017,  0.2864,  0.2205,  0.5464,  0.5819,  1.4917,  0.2004,  1.2909],\n",
      "        [ 0.8480, -0.3964, -0.6342, -1.8268, -1.4160,  0.1497,  0.3410, -0.3493],\n",
      "        [-1.1851, -0.0243,  0.3758,  0.6358,  1.3705, -1.2618,  0.7162,  0.0703],\n",
      "        [-0.1896,  0.8632,  0.4664,  1.3774,  0.3865, -0.9360, -0.6005, -0.0433],\n",
      "        [ 1.8587, -0.4916, -1.6394, -0.2646, -0.0746, -0.8691,  0.9307, -0.6233],\n",
      "        [ 0.6935,  2.0801,  0.1854, -2.1982, -1.2153, -0.5887,  0.7340, -0.8730],\n",
      "        [ 0.2346, -0.3757,  2.4206,  0.0755,  1.4496, -0.6230,  0.4157, -0.0909],\n",
      "        [ 1.4381,  0.2277, -0.6998,  1.1900, -2.1089, -0.1916, -0.7670,  0.5331],\n",
      "        [ 1.7124,  0.8573, -1.1487, -1.0287, -0.4410, -0.0521,  0.2638,  0.0575],\n",
      "        [ 2.7933, -1.2275, -0.2549, -0.6340,  1.7176, -0.9101,  0.8150,  2.3481],\n",
      "        [-1.8493,  0.9429, -0.2904, -1.2301,  0.8740, -1.5658,  2.2909, -0.2211]]),\n",
      "          'hier.0.bias': tensor([[ 0.6657,  0.8847,  0.4671, -0.6452, -0.4409, -0.3226, -0.9935, -0.8205,\n",
      "         -0.3198, -0.3102,  0.4122, -1.4471, -1.1796, -0.8329,  0.4536, -0.8209,\n",
      "          1.0106,  1.3373,  0.1781, -0.0192, -0.5101,  0.0324, -1.0560, -0.2062,\n",
      "          0.9719, -2.6161, -0.1014,  0.8617, -0.3116,  0.7709, -0.8416,  1.7962,\n",
      "          0.1924, -0.1777,  0.3214,  1.8793,  1.0058,  0.8708, -0.6902, -1.1933,\n",
      "          0.8315,  0.5742,  0.2648,  1.0264, -0.3700,  0.7571,  0.3196,  1.5331,\n",
      "          0.8183,  1.2512, -0.4571, -1.0858,  0.0431,  0.4952, -0.9355, -0.6429,\n",
      "         -1.2763, -0.6659, -0.1982,  2.1850, -0.0449, -1.1616, -0.5921,  0.7457]]),\n",
      "          'hier.0.weight': tensor([[-1.1258, -1.1524, -0.2506,  ...,  1.5863,  0.9463, -0.8437],\n",
      "        [-0.6136,  0.0316, -0.4927,  ..., -1.2341,  1.8197, -0.5515],\n",
      "        [-0.5692,  0.9200,  1.1108,  ..., -0.9565,  0.0335,  0.7101],\n",
      "        ...,\n",
      "        [ 2.2746, -0.9119,  0.5105,  ...,  0.4876, -0.9265, -0.5748],\n",
      "        [ 0.7300, -0.9287,  0.1743,  ..., -0.7073, -0.8813, -0.5895],\n",
      "        [-0.8363, -1.8354,  0.4765,  ..., -0.3812, -1.6687,  1.0869]]),\n",
      "          'hier.2.0.bias': tensor([[ 1.3992e+00, -6.9211e-01, -5.0479e-01,  1.1740e+00, -2.6585e-02,\n",
      "          6.3287e-01, -5.5119e-01, -2.3041e-01,  4.5340e-01,  9.5535e-01,\n",
      "          3.3961e-01, -1.3020e+00, -2.5281e-01, -1.4194e-01, -8.1444e-01,\n",
      "         -1.0294e-01,  3.6886e-01, -3.2871e-01, -3.2211e-01, -2.3720e-01,\n",
      "         -2.1999e-01,  4.1985e-01,  7.0288e-01,  1.5914e+00,  9.4751e-01,\n",
      "         -7.5783e-01,  5.6568e-02, -1.7967e-01,  1.5641e+00, -4.7876e-01,\n",
      "          4.5250e-01,  3.3698e-01, -1.9621e-01,  4.0187e-01,  9.8542e-01,\n",
      "         -1.5731e+00, -9.5435e-01, -9.9657e-01, -5.6711e-01,  1.0576e+00,\n",
      "          2.0924e+00, -4.3870e-01, -2.6173e+00, -2.9391e-01, -1.2045e+00,\n",
      "          3.9955e-01,  1.2171e+00,  9.2756e-01, -3.8234e-01, -2.0688e-01,\n",
      "          3.5718e-01,  1.5855e+00, -1.0789e+00, -2.0981e-01, -4.1862e-01,\n",
      "         -2.2013e+00,  7.6807e-01, -8.7934e-04, -8.4371e-01, -5.9478e-01,\n",
      "         -2.6929e+00, -2.0680e-01,  3.1771e-02, -1.7465e-01]]),\n",
      "          'hier.2.0.weight': tensor([[-1.1870, -0.8221,  0.6051,  ..., -0.5603,  0.6947, -1.1198],\n",
      "        [-0.0529,  0.7618, -0.7610,  ..., -0.8519, -0.0543, -0.8025],\n",
      "        [-1.1104,  0.2151, -0.3594,  ...,  0.4093,  1.3281, -1.0375],\n",
      "        ...,\n",
      "        [ 0.3974, -0.1886,  1.0552,  ..., -1.7816,  1.5874, -1.2078],\n",
      "        [-0.7160, -2.0093, -0.8437,  ...,  0.3333, -0.2713,  0.0072],\n",
      "        [ 0.8539,  0.8351,  0.7582,  ..., -0.3573,  0.4395,  0.5350]]),\n",
      "          'hier.3.0.bias': tensor([[-0.5077, -0.3653, -0.9776, -0.1630,  0.3506, -0.9446, -0.8569,  0.6887,\n",
      "          1.6695,  0.1223, -1.8314, -1.1037,  0.0247,  0.1930, -0.8428, -0.4777,\n",
      "          1.6149,  0.7442,  0.7745,  0.4676,  0.3437,  0.6474,  0.3575,  0.2432,\n",
      "         -0.4112,  1.7646, -1.2207, -0.9906, -0.6231, -0.5218,  1.4282, -0.9732,\n",
      "         -0.0968, -0.5241, -0.6659, -1.1256, -0.9553, -0.3100,  0.0127,  0.7653,\n",
      "         -0.0231, -0.5024,  0.8051, -1.9528, -0.3265,  0.3357, -0.3324,  0.9156,\n",
      "         -0.5095,  0.5706, -0.4278, -0.1623, -0.7230, -0.0087,  0.7899,  2.2654,\n",
      "         -0.5406,  1.2762,  0.8371,  1.1455,  0.5012,  1.7536, -2.5398,  0.3686]]),\n",
      "          'hier.3.0.weight': tensor([[-0.8944, -0.2306, -0.0215,  ..., -0.7445, -1.6360, -1.3502],\n",
      "        [-1.0278, -0.6498, -3.1510,  ..., -0.7780,  0.5115, -0.5564],\n",
      "        [ 1.4164,  0.2379, -0.9334,  ..., -0.0097, -2.3913,  0.3396],\n",
      "        ...,\n",
      "        [-0.8602,  0.1470, -0.6346,  ..., -0.4884, -0.1077, -0.3000],\n",
      "        [-1.2771, -0.1230,  0.2431,  ..., -0.7051,  0.4487, -0.8186],\n",
      "        [ 0.6829, -0.5540,  1.0106,  ..., -0.4571,  0.9002, -0.1539]])},\n",
      " 'last': {'beta': tensor([[ 2.2053,  4.6185,  0.8132, -0.4132, -1.8075, -1.8218, -3.9480,  1.6013],\n",
      "        [-0.1068, -1.9270, -0.9956,  2.4985, -2.6432,  1.2277, -1.1753,  4.5100],\n",
      "        [ 2.5796,  0.7192,  1.0172,  1.7735,  1.3341,  2.3647, -1.0597, -1.2376],\n",
      "        [-1.2865, -2.7418, -1.6722,  0.2111,  4.1375, -0.1163,  2.4159, -1.8229],\n",
      "        [-4.0816, -3.7890, -0.7269, -1.0105,  0.0233,  1.4735,  1.9468,  2.9280],\n",
      "        [-0.1449,  0.3500, -0.0793,  0.7873,  0.0475, -1.0988,  1.6796, -1.4431],\n",
      "        [ 1.8362,  0.3612,  1.2219, -2.7678,  1.7951, -1.5111, -1.8773, -2.2699],\n",
      "        [-2.7937, -1.8271,  3.6227, -1.2140, -2.3245,  5.7113, -4.3195,  1.2057],\n",
      "        [-0.3909,  4.8784,  1.3417, -2.7757, -1.6695,  1.1181,  3.1647, -3.7635],\n",
      "        [ 0.7134, -0.4670,  0.8454,  0.9793,  1.3233, -1.2105, -2.6052,  3.3964],\n",
      "        [ 0.5636, -0.5145,  1.0696,  2.3236, -0.7160, -1.5006,  0.3061,  0.0366],\n",
      "        [-0.0751, -0.2950, -0.2626,  0.7907,  0.7015,  0.8781, -0.9444,  0.2616],\n",
      "        [ 1.4284, -2.9438, -1.7443, -3.3032,  1.2322, -2.6757, -0.3766, -0.6059],\n",
      "        [ 0.2784,  0.4422, -0.5369, -2.4422,  1.5661,  0.7570,  0.3073,  2.0430],\n",
      "        [ 1.8047, -2.9288, -3.6225, -0.2854, -0.4082,  1.3212,  1.8992,  4.0843],\n",
      "        [ 0.5812,  2.2940, -0.9944,  1.0582, -0.9952, -0.6442, -1.0342, -2.6086],\n",
      "        [ 0.2439, -1.8091, -0.8632, -0.7270, -2.0425,  1.6103,  1.7691,  2.3850],\n",
      "        [ 7.7204,  2.5914, -5.4074,  3.1343, -0.2427, -8.9969, -3.3726,  3.8356],\n",
      "        [-4.3740, -5.0844,  3.5492,  1.8778, -1.5090,  3.4262, -1.3244,  0.7841],\n",
      "        [-0.9906, -1.2904,  0.2638, -0.6461, -2.4409, -0.4233,  0.2238,  0.0435],\n",
      "        [-4.4659, -5.6121,  0.3300,  1.4546,  0.9909,  6.7511,  4.6547, -0.6201],\n",
      "        [-2.3837,  1.3228,  1.9072,  0.8347,  3.4888,  1.7973,  2.0240, -5.5480],\n",
      "        [ 0.1425, -1.4259, -1.7786, -1.7512,  0.7213, -1.1133,  0.5715,  0.1379],\n",
      "        [-2.8686,  2.9875, -1.7357,  2.7645, -1.4601, -0.5046, -1.3660, -0.5677],\n",
      "        [-0.9345,  0.9914, -1.1868, -0.0562, -1.4263,  2.0304, -1.2408,  0.9662],\n",
      "        [-3.9815, -2.2932, -2.6439,  2.7311,  1.1786,  1.8726,  1.5902,  1.4370],\n",
      "        [-0.3667, -0.3086, -2.4005, -0.3247, -0.9073,  0.9377, -0.7950, -0.5310],\n",
      "        [ 1.3109,  1.2354, -0.9052, -0.8592, -0.6565, -0.6840,  0.3328, -0.6917],\n",
      "        [-0.1648, -0.4143,  1.3043,  0.8568, -0.7681, -0.3728,  0.0423,  1.4078],\n",
      "        [ 2.6822, -2.6119, -2.3460, -1.9623, -0.2116, -2.5182,  0.6850,  1.0584],\n",
      "        [ 4.4213, -0.8821, -2.6505,  0.8014, -4.3682, -2.9642, -3.4153,  5.6460],\n",
      "        [ 3.6645,  3.2836, -3.4654,  0.7700, -0.4642, -3.0806, -1.4434, -3.8757],\n",
      "        [ 0.1521,  0.2171, -1.1609,  0.1219,  0.7656, -1.1512, -0.5717, -1.3921],\n",
      "        [ 1.7392, -8.8553, -3.9847,  0.9437, -1.2657,  4.2831,  4.1917,  4.5690],\n",
      "        [ 0.5938, -3.0361, -1.3697,  2.1232, -1.6693, -2.6782,  2.3305,  3.6946],\n",
      "        [-1.7123,  1.2989,  4.3976, -2.8465, -0.4400,  3.1503, -1.6957, -3.1163],\n",
      "        [-0.3823,  0.6876,  0.1598, -0.3373, -0.0992, -1.0827, -0.9008,  0.2326],\n",
      "        [ 0.1649, -2.6292, -3.2470,  0.3683, -0.0336,  3.2569,  2.9544, -0.1408],\n",
      "        [-0.6938,  0.3271,  0.9745, -0.0994, -0.2798, -1.4162, -1.6128, -1.9073],\n",
      "        [ 0.1883,  3.5096, -1.5500,  3.6661,  0.0485, -1.9910,  2.6752, -0.9880],\n",
      "        [ 3.1656,  1.4093, -1.0466, -0.8737,  0.9932, -1.1136, -2.2153, -2.0505],\n",
      "        [-1.1925, -1.0966, -1.6364,  2.4428, -0.5416, -1.7384,  1.8527,  0.6870],\n",
      "        [ 0.0763,  3.7381,  0.2007,  1.4479, -0.0481, -2.1506, -1.4208, -0.3908],\n",
      "        [ 1.0669, -0.2995, -1.9296,  1.0296, -0.7970, -0.9851,  0.2247,  0.9283],\n",
      "        [ 0.7104, -0.0855,  3.0638,  1.7870,  0.4382,  0.0647,  0.4664,  0.3387],\n",
      "        [ 0.5330,  0.9791,  5.5199,  1.7526,  2.4063, -2.1765, -4.0071, -5.9419],\n",
      "        [-0.0205,  1.2629, -1.2765,  0.6727,  0.1462, -0.9395, -1.1528,  0.5291],\n",
      "        [-1.3669,  5.7749,  2.7529,  1.0517,  4.9163, -2.7476, -3.4897, -4.4705],\n",
      "        [-1.4300,  0.8473,  0.6531, -1.0793,  0.6209, -0.2510,  1.0024,  0.8016],\n",
      "        [-0.5122,  0.8879, -1.2634,  1.2868,  0.0554, -0.2248,  1.4207, -1.7246],\n",
      "        [ 0.2515,  1.1049, -0.3244,  0.9865, -0.0210, -0.4413, -0.9151, -0.1386],\n",
      "        [ 1.4002, -2.1831, -2.2698, -1.3681, -1.3177,  0.9326,  5.8294, -1.1339],\n",
      "        [-3.3632,  0.3439,  2.8941,  0.5105,  1.6459,  4.0274, -0.7797, -5.1525],\n",
      "        [-1.3361,  0.2584,  0.1961,  0.4591,  0.7090,  1.4113,  0.1393,  1.2430],\n",
      "        [ 5.3269, -2.7763, -2.4461, -3.1553, -2.1295, -0.1379,  2.2570, -0.1843],\n",
      "        [-2.1822, -1.3702,  1.0196,  0.4703,  3.2528, -0.6945,  1.4157, -1.2224],\n",
      "        [-2.8226,  2.4597,  5.6829,  1.8613,  1.8763, -0.0558, -4.7603, -2.9329],\n",
      "        [ 1.5737, -0.5548, -2.6336,  0.2067, -0.3618, -0.9220,  2.9548, -1.4224],\n",
      "        [ 0.2904,  1.9174,  2.1846, -2.5546, -1.9185,  0.4344, -0.2028, -1.3193],\n",
      "        [-1.7839, -1.2116,  4.0724,  0.0669,  3.3621,  1.2424, -0.0827, -2.2003],\n",
      "        [ 1.9159,  4.0602, -2.2962,  3.6716, -2.2523, -2.9138, -1.3726, -1.1869],\n",
      "        [ 4.5375,  1.0943, -3.3913, -1.9354, -1.5924, -2.6331,  2.5624,  1.5760],\n",
      "        [ 2.7607, -1.2132, -0.2519, -0.6266,  1.6976, -0.8994,  0.8055,  2.3206],\n",
      "        [-4.9097,  2.5119, -0.0688, -1.2696,  5.6887, -1.2967,  3.1199, -4.8124]]),\n",
      "          'hier.0.bias': tensor([[ 1.1724,  1.7220,  1.7760,  0.5883, -0.3491, -0.4659, -0.6721,  0.0893,\n",
      "          0.6005,  0.3321,  0.7293, -0.7655,  0.6018,  0.0155,  2.7158, -0.6913,\n",
      "          1.0641,  3.3539,  0.5277,  1.1587,  1.3733,  1.1878, -0.1070,  0.6199,\n",
      "          1.7443, -2.4533,  0.1661,  2.7289,  0.6429,  1.1456, -0.6387,  2.1082,\n",
      "          0.2449,  0.7076,  0.6025,  2.7189,  1.4316,  1.6283, -0.2382,  0.2440,\n",
      "          2.7555,  1.7396,  0.2837,  3.2084, -0.4201,  1.9013,  2.1366,  2.1837,\n",
      "          1.3016,  2.1445, -0.2932, -0.7116,  2.2566,  1.3767,  0.4184,  0.6586,\n",
      "         -0.4139, -0.2783, -0.1750,  1.9408,  1.2330, -0.3586, -0.4469,  1.4679]]),\n",
      "          'hier.0.weight': tensor([[-1.3957, -1.1490,  0.1527,  ...,  1.6926,  0.6818, -1.0331],\n",
      "        [-0.2009, -0.2805, -0.3974,  ..., -1.2284,  2.4086,  0.0443],\n",
      "        [-1.9445,  1.0197,  2.4756,  ..., -1.0623,  0.7495,  1.4241],\n",
      "        ...,\n",
      "        [ 3.1949, -0.2490,  1.2730,  ...,  0.1635, -0.6970, -0.7367],\n",
      "        [ 0.3256, -0.9260,  0.2045,  ..., -0.6539, -0.8542, -0.6840],\n",
      "        [-0.7780, -2.1788,  0.2858,  ..., -0.2713, -1.3676,  1.4192]]),\n",
      "          'hier.2.0.bias': tensor([[ 1.7186e+00, -3.0568e-01, -2.8445e-01,  1.4951e+00,  4.7258e-01,\n",
      "          1.7123e+00,  1.6825e-01,  6.0591e-02,  1.7543e+00,  1.4482e+00,\n",
      "          1.0670e+00, -8.6815e-01,  5.9231e-01,  3.4579e-01, -3.7827e-01,\n",
      "          2.3295e-01,  3.8903e-01,  1.5993e-01, -6.5038e-03, -2.2272e-01,\n",
      "          8.5488e-01,  1.3851e+00,  1.3128e+00,  1.7142e+00,  1.2996e+00,\n",
      "         -7.0991e-01,  2.7362e-01,  1.3436e-01,  2.4711e+00, -4.5948e-01,\n",
      "          2.9670e-01,  1.0996e+00, -5.4521e-04,  2.0158e+00,  1.4443e+00,\n",
      "         -1.3216e+00, -8.6824e-01, -7.6172e-01, -1.2922e-01,  1.3983e+00,\n",
      "          2.7863e+00,  9.6531e-02, -2.0073e+00,  9.1774e-01, -2.6644e-01,\n",
      "          1.2064e+00,  1.0104e+00,  7.0351e-01,  2.6640e-01,  8.1768e-01,\n",
      "          2.0177e+00,  1.9408e+00,  2.1029e-01,  3.7385e-01,  4.7613e-01,\n",
      "         -1.7416e+00,  1.4865e+00,  4.2355e-01, -7.9947e-01,  2.6592e-01,\n",
      "         -2.3108e+00, -2.7039e-02,  3.5028e-01, -1.1975e-01]]),\n",
      "          'hier.2.0.weight': tensor([[-1.2217, -0.6157,  1.4543,  ..., -0.2528,  0.3754, -1.1028],\n",
      "        [ 0.0076,  0.6627, -0.4385,  ..., -0.9817, -0.0792, -0.9519],\n",
      "        [-1.0221,  0.2298, -0.3575,  ...,  0.4087,  1.3861, -1.0066],\n",
      "        ...,\n",
      "        [ 0.5754, -0.2615,  1.5811,  ..., -2.0603,  1.6976, -1.1473],\n",
      "        [-0.6899, -1.7881, -0.6271,  ...,  0.6203, -0.2836,  0.0958],\n",
      "        [ 1.0630,  0.8490,  1.4608,  ..., -0.3830,  0.3084,  0.7164]]),\n",
      "          'hier.3.0.bias': tensor([[ 0.0129,  0.5564, -0.8483,  0.0239,  0.4749, -0.8594, -0.8302,  1.0013,\n",
      "          1.7837,  0.6676, -1.8622, -1.0041,  0.1381,  0.3484, -0.6135, -0.1590,\n",
      "          1.3860,  2.1229,  1.4889,  0.4292,  0.8631,  1.1392,  0.0982,  0.7336,\n",
      "         -0.8379,  2.4512, -1.1106, -0.9012, -0.6400, -0.3558,  1.6820, -0.2934,\n",
      "         -0.1021,  0.4264,  0.0461, -0.5706, -0.9232, -0.0687,  0.0961,  1.1653,\n",
      "         -0.0146, -0.0256,  1.0421, -1.7766, -0.4948,  1.0682, -0.3296,  1.6883,\n",
      "         -0.4551,  0.6081, -0.5084,  0.0537, -0.3928, -0.1928,  1.0779,  1.9711,\n",
      "         -0.1442,  1.5584,  0.9346,  1.6655,  1.1018,  1.7174, -2.3107,  1.6077]]),\n",
      "          'hier.3.0.weight': tensor([[-1.8334, -0.8380,  0.0219,  ..., -0.2118, -1.7928, -0.7184],\n",
      "        [-1.5177, -0.5736, -3.0719,  ..., -0.7337,  0.4274, -0.6838],\n",
      "        [ 1.5223,  0.0257, -0.9582,  ...,  0.1406, -2.3598,  0.5393],\n",
      "        ...,\n",
      "        [-0.4872,  0.7849, -0.6568,  ...,  0.0218, -0.2816,  0.8229],\n",
      "        [-1.2752, -0.1229,  0.2428,  ..., -0.7041,  0.4481, -0.8174],\n",
      "        [ 2.1295,  0.2267,  0.9888,  ..., -0.8009,  1.1483,  0.7832]])},\n",
      " 'locality': [],\n",
      " 'stability': [],\n",
      " 'terr': [75.0,\n",
      "          75.0,\n",
      "          66.66666666666666,\n",
      "          66.66666666666666,\n",
      "          66.66666666666666,\n",
      "          66.66666666666666,\n",
      "          66.66666666666666,\n",
      "          83.33333333333333,\n",
      "          83.33333333333333,\n",
      "          66.66666666666666,\n",
      "          66.66666666666666,\n",
      "          66.66666666666666,\n",
      "          58.333333333333336,\n",
      "          41.666666666666664,\n",
      "          25.0,\n",
      "          25.0,\n",
      "          25.0,\n",
      "          25.0,\n",
      "          25.0,\n",
      "          25.0,\n",
      "          25.0,\n",
      "          25.0,\n",
      "          25.0,\n",
      "          25.0,\n",
      "          25.0],\n",
      " 'train loss': [2.0812392234802246,\n",
      "                2.0811073780059814,\n",
      "                2.0808591842651367,\n",
      "                2.08050799369812,\n",
      "                2.0800650119781494,\n",
      "                2.0795440673828125,\n",
      "                2.078953266143799,\n",
      "                2.078300714492798,\n",
      "                2.077592611312866,\n",
      "                2.0768306255340576,\n",
      "                2.076031446456909,\n",
      "                2.075205087661743,\n",
      "                2.074355363845825,\n",
      "                2.0734901428222656,\n",
      "                2.0726139545440674,\n",
      "                2.0717408657073975,\n",
      "                2.0708587169647217,\n",
      "                2.0699665546417236,\n",
      "                2.0690507888793945,\n",
      "                2.0681145191192627,\n",
      "                2.0671534538269043,\n",
      "                2.066169023513794,\n",
      "                2.0651583671569824,\n",
      "                2.0641138553619385,\n",
      "                2.063039541244507,\n",
      "                2.06192684173584,\n",
      "                2.0607690811157227,\n",
      "                2.059562921524048,\n",
      "                2.0583157539367676,\n",
      "                2.057013511657715,\n",
      "                2.0556492805480957,\n",
      "                2.05421781539917,\n",
      "                2.052718162536621,\n",
      "                2.05115008354187,\n",
      "                2.0495007038116455,\n",
      "                2.0477702617645264,\n",
      "                2.045948028564453,\n",
      "                2.0440382957458496,\n",
      "                2.0420196056365967,\n",
      "                2.039881944656372,\n",
      "                2.037623405456543,\n",
      "                2.0352282524108887,\n",
      "                2.03271484375,\n",
      "                2.0300588607788086,\n",
      "                2.0272374153137207,\n",
      "                2.0242388248443604,\n",
      "                2.0210559368133545,\n",
      "                2.0176663398742676,\n",
      "                2.014065742492676,\n",
      "                2.0102286338806152,\n",
      "                2.00614595413208,\n",
      "                2.00180721282959,\n",
      "                1.9971678256988525,\n",
      "                1.9921826124191284,\n",
      "                1.9868355989456177,\n",
      "                1.981094241142273,\n",
      "                1.9749616384506226,\n",
      "                1.9684230089187622,\n",
      "                1.9614120721817017,\n",
      "                1.9538944959640503,\n",
      "                1.9458225965499878,\n",
      "                1.9371650218963623,\n",
      "                1.9278850555419922,\n",
      "                1.9179576635360718,\n",
      "                1.907338261604309,\n",
      "                1.8959563970565796,\n",
      "                1.8837703466415405,\n",
      "                1.8707181215286255,\n",
      "                1.8567501306533813,\n",
      "                1.841806173324585,\n",
      "                1.8258366584777832,\n",
      "                1.8088314533233643,\n",
      "                1.7907402515411377,\n",
      "                1.7715383768081665,\n",
      "                1.7511872053146362,\n",
      "                1.729627013206482,\n",
      "                1.706804871559143,\n",
      "                1.6827516555786133,\n",
      "                1.6575061082839966,\n",
      "                1.6310724020004272,\n",
      "                1.603487253189087,\n",
      "                1.5748687982559204,\n",
      "                1.545314073562622,\n",
      "                1.5149364471435547,\n",
      "                1.4839565753936768,\n",
      "                1.452555775642395,\n",
      "                1.420965552330017,\n",
      "                1.3894373178482056,\n",
      "                1.35812509059906,\n",
      "                1.3271361589431763,\n",
      "                1.2965809106826782,\n",
      "                1.266472578048706,\n",
      "                1.236825704574585,\n",
      "                1.2076650857925415,\n",
      "                1.1790307760238647,\n",
      "                1.1509641408920288,\n",
      "                1.1235966682434082,\n",
      "                1.0969526767730713,\n",
      "                1.0710861682891846,\n",
      "                1.0460071563720703,\n",
      "                1.0218021869659424,\n",
      "                0.998429536819458,\n",
      "                0.9756813049316406,\n",
      "                0.9534702897071838,\n",
      "                0.9317986965179443,\n",
      "                0.9106855988502502,\n",
      "                0.8901489973068237,\n",
      "                0.8701253533363342,\n",
      "                0.8506003022193909,\n",
      "                0.8316268920898438,\n",
      "                0.8132323622703552,\n",
      "                0.7954729199409485,\n",
      "                0.7782614231109619,\n",
      "                0.7615916728973389,\n",
      "                0.7454056739807129,\n",
      "                0.7295987606048584,\n",
      "                0.7141749858856201,\n",
      "                0.6990780830383301,\n",
      "                0.6843242049217224,\n",
      "                0.6699134707450867,\n",
      "                0.6558550596237183,\n",
      "                0.6421908140182495,\n",
      "                0.6289017796516418,\n",
      "                0.6160386800765991,\n",
      "                0.6036120653152466,\n",
      "                0.5916117429733276,\n",
      "                0.580012321472168,\n",
      "                0.5688098669052124,\n",
      "                0.5579975843429565,\n",
      "                0.5475690960884094,\n",
      "                0.5375057458877563,\n",
      "                0.5278240442276001,\n",
      "                0.5185120701789856,\n",
      "                0.5095152258872986,\n",
      "                0.500851035118103,\n",
      "                0.4925030469894409,\n",
      "                0.48446306586265564,\n",
      "                0.47673898935317993,\n",
      "                0.46931731700897217,\n",
      "                0.4622006416320801,\n",
      "                0.45536544919013977,\n",
      "                0.448801189661026,\n",
      "                0.4425114393234253,\n",
      "                0.4364844262599945,\n",
      "                0.43071332573890686,\n",
      "                0.42518478631973267,\n",
      "                0.41988635063171387,\n",
      "                0.41482773423194885,\n",
      "                0.4100039005279541,\n",
      "                0.4054000675678253,\n",
      "                0.4010106027126312,\n",
      "                0.39682096242904663,\n",
      "                0.3928128480911255,\n",
      "                0.3889838457107544,\n",
      "                0.3853292763233185,\n",
      "                0.381848007440567,\n",
      "                0.37854060530662537,\n",
      "                0.37539437413215637,\n",
      "                0.37240031361579895,\n",
      "                0.36955681443214417,\n",
      "                0.36685970425605774,\n",
      "                0.3642968237400055,\n",
      "                0.36187219619750977,\n",
      "                0.3595823347568512,\n",
      "                0.35741493105888367,\n",
      "                0.3553619682788849,\n",
      "                0.35342535376548767,\n",
      "                0.3515975773334503,\n",
      "                0.3498777449131012,\n",
      "                0.34826478362083435,\n",
      "                0.34675395488739014,\n",
      "                0.3453417420387268,\n",
      "                0.3440234363079071,\n",
      "                0.3427964448928833,\n",
      "                0.341656357049942,\n",
      "                0.3406003415584564,\n",
      "                0.3396252393722534,\n",
      "                0.3387291729450226,\n",
      "                0.3379054367542267,\n",
      "                0.33715155720710754,\n",
      "                0.33646512031555176,\n",
      "                0.3358425796031952,\n",
      "                0.335281103849411,\n",
      "                0.33477649092674255,\n",
      "                0.33432623744010925,\n",
      "                0.33392733335494995,\n",
      "                0.33357667922973633,\n",
      "                0.33327144384384155,\n",
      "                0.3330082893371582,\n",
      "                0.33278417587280273,\n",
      "                0.33259424567222595,\n",
      "                0.33243680000305176,\n",
      "                0.33230894804000854,\n",
      "                0.3322076201438904,\n",
      "                0.3321300446987152,\n",
      "                0.33207303285598755,\n",
      "                0.33203333616256714,\n",
      "                0.3320079445838928,\n",
      "                0.33199357986450195,\n",
      "                0.3319872319698334,\n",
      "                0.3319856524467468,\n",
      "                0.33198562264442444,\n",
      "                0.3319840133190155,\n",
      "                0.33197760581970215,\n",
      "                0.33196327090263367,\n",
      "                0.331937700510025,\n",
      "                0.3318978548049927,\n",
      "                0.33184051513671875,\n",
      "                0.33176249265670776,\n",
      "                0.33166053891181946,\n",
      "                0.33153149485588074,\n",
      "                0.3313722014427185,\n",
      "                0.3311796486377716,\n",
      "                0.33095070719718933,\n",
      "                0.3306821584701538,\n",
      "                0.3303706645965576,\n",
      "                0.33001357316970825,\n",
      "                0.32960808277130127,\n",
      "                0.32915106415748596,\n",
      "                0.32863909006118774,\n",
      "                0.32807010412216187,\n",
      "                0.32744067907333374,\n",
      "                0.32674771547317505,\n",
      "                0.3259890675544739,\n",
      "                0.3251616954803467,\n",
      "                0.3242625296115875,\n",
      "                0.3232891261577606,\n",
      "                0.322239488363266,\n",
      "                0.321111261844635,\n",
      "                0.31990113854408264,\n",
      "                0.31860724091529846,\n",
      "                0.31722718477249146,\n",
      "                0.3157588839530945,\n",
      "                0.3142002820968628,\n",
      "                0.3125486969947815,\n",
      "                0.31080150604248047,\n",
      "                0.3089583218097687,\n",
      "                0.3070164620876312,\n",
      "                0.3049755394458771,\n",
      "                0.30283379554748535,\n",
      "                0.3005886375904083,\n",
      "                0.2982407510280609,\n",
      "                0.2957835793495178,\n",
      "                0.2932056188583374,\n",
      "                0.29052209854125977,\n",
      "                0.28774261474609375,\n",
      "                0.28486180305480957,\n",
      "                0.28187698125839233,\n",
      "                0.27879974246025085,\n",
      "                0.2756187915802002],\n",
      " 'weight_evo': tensor(0.8281)}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "def loadall(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                yield pickle.load(f)\n",
    "            except EOFError:\n",
    "                break\n",
    "\n",
    "items = loadall(\"hierarchy-learning/test.pkl\")\n",
    "\n",
    "for item in items:\n",
    "    pprint(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
