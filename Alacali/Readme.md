# IFL-GAN: Improved Federated Learning Generative Adversarial Network With Maximum Mean Discrepancy Model Aggregation

This readme file is an outcome of the [CENG501 (Fall 2024)](https://ceng.metu.edu.tr/~skalkan/DL/) project for reproducing a paper without an implementation. See [CENG501 (Fall 2024) Project List](https://github.com/CENG501-Projects/CENG501-Fall2024) for a complete list of all paper reproduction projects.

# 1. Introduction
Generative Adversarial Networks (GANs) are powerful generative models that simulate data distributions by training a generator-discriminator pair in a competitive setting. However, traditional GANs often require centralized and independent, identically distributed (i.i.d.) training data, which is impractical in many real-world scenarios. Data is often distributed among multiple clients and is non-i.i.d., posing challenges for effective GAN training.

Federated Learning (FL) has been applied to address this issue by training GANs in a decentralized manner. Existing approaches, like Federated Learning GAN (FL-GAN), aggregate updates using Federated Averaging (FedAvg), but this method suffers in non-i.i.d. cases, leading to poor convergence and low-quality outputs.

This paper proposes Improved Federated Learning GAN (IFL-GAN), which uses Maximum Mean Discrepancy (MMD) to aggregate updates, assigning weights based on local GAN convergence status. This approach achieves faster training convergence and higher-quality outputs on non-i.i.d. datasets.

## 1.1 Paper Summary
The contributions of the paper can be summarized as:

- A novel framework called IFL-GAN that replaces federating average strategy (FedAvg) with MMD for aggregating local GAN updates, improving performance in non-i.i.d. data settings.
- Theoretical and empirical comparisons of MMD and FedAvg aggregation methods.
- Extensive experimental validation showing superior performance in terms of data diversity and quality across MNIST, CIFAR10, and SVHN datasets.
The IFL-GAN system model is depicted in Figure 1:

<p align="center">
  <img src="figures/system model.png" style="width: 70%;"><br>
  <em>Figure 1: IFL-GAN system model</em>
</p>
 

# 2. Methodology
## 2.1. Key Concepts

### Generative Adversarial Networks
Generative Adversarial Networks (GANs) consist of two neural networks: a Generator (G) and a Discriminator (D), trained in an adversarial manner. The generator creates synthetic data, while the discriminator attempts to distinguish between real and generated data. The GAN objective function is:

$$min_G  max_D V(G, D) = E_{x ~~ p_r(x)} [log D(x)] + E_{z ~~ p_z(z)} [log (1 - D(G(z)))]$$

Where:

$D(x)$: Probability that x is a real data sample.

$G(z)$: Synthetic data generated from input noise z.

$p_r(x)$: Real data distribution.

$p_z(z)$: easy-to-sample Noise distribution (e.g., Gaussian or uniform).

### Federated Learning for GANs
Federated Learning (FL) enables decentralized training across multiple clients without transferring raw data, preserving privacy. Instead, clients train local models and share their updates with a central server. However, training GANs in a federated setting introduces challenges such as:

- Non-i.i.d. Data: Data across clients often have unique distributions.

- Stability Issues: The adversarial nature of GANs makes distributed training more complex.
FL-GAN extends federated learning to GANs by aggregating updates from multiple local GANs to train a global GAN.

### Federated Learning GAN (FL-GAN)
FL-GAN uses a Federated Averaging (FedAvg) strategy to aggregate updates from local GANs into a global GAN model. Each client maintains a local GAN, with a generator (G_i) and a discriminator (D_i).

FL-GAN Objective Function:
The FL-GAN training objective is:

$min_G max_D V(G, D) = (1/K) * Σ_{i=1}^{K} [E_{x ~ p_{r_i}(x)} log D_i(x) + E_{z ~ p_z(z)} log (1 - D_i(G_i(z)))]$

Where:

$K$: Number of clients.

$G_i, D_i$: Local generator and discriminator for client i.

$p_{r_i}(x)$: Real data distribution for client i.

$G_i(z)$: Data generated by the local generator for client i.

### Challenges in FL-GAN
Despite its utility, according to the writers FL-GAN faces several challenges:

Non-i.i.d. Data: FedAvg assumes equal contribution from all clients, which is ineffective when data distributions vary.

Convergence Instability: Aggregating updates from clients with different convergence statuses can destabilize training.

Low Diversity: Uniform weighting may reduce diversity and fidelity in generated data.

These limitations motivate the development of Improved Federated Learning GAN (IFL-GAN), which uses Maximum Mean Discrepancy (MMD) for weighted aggregation to address these issues effectively.

## 2.1.1 IFL-GAN

IFL-GAN introduces an MMD-based mechanism to aggregate local GAN updates, assigning weights dynamically based on each local GAN's convergence status. 

$min_G max_D V(G, D) = (1/K) * Σ_{i=1}^{K} α_i * [E_{x ~ p_{r_i}(x)} log D_i(x) + E_{z ~ p_z(z)} log (1 - D_i(G_i(z)))]$

Unlike traditional Federated Averaging (FedAvg), which equally weights updates ($α_i={1/K}$ for all i), this method ensures that poorly converged models contribute more to the global model, leading to better stability and quality of generated data in non-i.i.d. settings. The pseudocode for the algorithm is shared in Figure 2.

<p align="center">
  <img src="figures/pseudocode for the algorithm.png" style="width: 70%;"><br>
  <em>Figure 2: IFL-GAN Algorithm</em>
</p>

## 2.1.2. IFL-GAN Algorithm Steps

Step 1: Initialization
A global GAN model is initialized, consisting of a generator $G_{global}$, and a discriminator $D_{global}$. The global model parameters are distributed to all $K$ clients.

Step 2: Local Training
Each client trains its local GAN on its private, potentially non-i.i.d. dataset: 

1. For client $i$, a local generator $G_i$ and a discriminator $D_i$ are initialized with the global model parameters.
   
2. The local GAN is trained using the standard GAN objective function:
   
3. After a fixed number of local training epochs, the client sends the updated parameters $\theta_{G_i}$ (from the generator) and $\theta_{D_i}$(from the discriminator) to the central server.

Step 3: MMD-Based Model Aggregation
The server aggregates the local model updates using Maximum Mean Discrepancy (MMD):

1. Compute the MMD score for each client's local generator: $MMD_i = MMD(p_{r_{\text{global}}}, p_{r_i})$
2. Convert MMD scores into aggregation weights: $α_i = exp(-MMD_i) / Σ_{j=1}^{K} exp(-MMD_j)$
3. Update the global generator parameters: $θ_{G_{\text{global}}} = Σ_{i=1}^{K} α_i * θ_{G_i}$

Step 4: Discriminator Aggregation
For the discriminator, the server aggregates updates similarly, though discriminator-specific adjustments may apply.

Step 5: Synchronization
The updated global generator $G_{global}$ and discriminator $D_{global}$ are sent back to all clients, replacing their local models:
$G_i = G_{\text{global}}, D_i = D_{\text{global}}, for i in [1, K]$

## 2.2. Our interpretation

  The overall aim of the work is represented thoroughly in the paper. However, the basic model that is tried to be improved upon, namely the FL-GAN is not discussed algorithm-wise. So it has been decided that to create a better understanding of the idea a more modular approach should be followed. In this part, the nuances will be discussed and in the experimental setup part (Part 3), the modularity will be shared.

1. The Discussion of the Method of FL-GAN
  As discussed in the previous chapters, the aim of studying GANs using the framework of federated learning is to fully utilize the distributed data across different devices for learning purposes. Thus, the results obtained from FL-GAN are used as a benchmark for the performance comparison of IFL-GAN in the paper. As this is going to be the first GAN and Federated Learning implementation of the author of this repository, it is decided that the GAN structure should be studied briefly. After that, the iid local dataset division, client model aggregation and global parameter distribution mechanisms are decided to be introduced to the basic model for FL-GAN application.

2. Shared Architectural Details
   In figure 3, the architectural details used in the study are shared. In detail, the IFL-GAN framework has been tried on three different datasets: MNIST, CIFAR10, and SVHN datasets. These are divided into non-iid groups to simulate local datasets of the users. However, according to the supposed data input dimensions, the architectures shared in figure 3 are not consistent. For instance, a data from MNIST dataset is a 28x28 grayscale image but the discriminator architecture is started with a 1x28 fully connected layer. So, it is decided that to get ahold of the GAN model first, a proven GAN structure is going to be used until the original idea is understood. Also, in [2], deep convolutional networks are introduced such that fully connected hidden layers are removed for deeper architectures. Such models can be trained on edge devices for better performance as well.

   <p align="center">
  <img src="figures/Architectural Details.png" style="width: 70%;"><br>
  <em>Figure 3: Architectural Details of IFL-GAN </em>
</p>


# 3. Experiments and results

## 3.1. Experimental setup

As shared in part 2.2, it is decided that since the IFL-GAN consists of a number of locally trained GANs, firstly some variations of different GAN networks are studied. 
### First Experiment with Central GAN

#### Objective and Architecture
Generator: Converts random noise (NOISE_DIM=100) into realistic 28x28 images using layers of linear transformations, reshaping, transposed convolutions, batch normalization, ReLU, and Tanh activations.

Discriminator: Distinguishes real images from fake ones using convolutional layers, batch normalization, and LeakyReLU activation, outputting a single probability value.

#### Experimental Setup
Dataset: MNIST, normalized to [-1, 1].
Hyperparameters: Learning rate = 0.0002, batch size = 256, Adam optimizer with betas = (0.5, 0.999).

#### Training Procedure
- Train Discriminator:

Calculate loss (real_loss) for real images.
Calculate loss (fake_loss) for fake images.
Minimize combined loss (real_loss + fake_loss).

- Train Generator:

Generate fake images and calculate loss (gen_loss) based on discriminator output.
Update the generator to maximize the discriminator’s error on fake images.

#### Hypothesis
Loss Trends:
Discriminator loss is expected to start high relatively, then decreases as it improves.
Generator loss decreases as generated images become realistic.
In the ideal case, both losses should converge to 50%.

#### Results

1- Generator and Discriminator Losses during 100 Epochs
![](https://github.com/CENG501-Projects/CENG501-Fall2024/blob/main/Alacali/figures/central_gan_1st_try.gif)

2- Images after 2nd, 11th and 100th Epochs


<div style="display: flex; flex-direction: row; gap: 20px;">
    <img src="figures/image_at_epoch_0002.png" style="width: 200px;">
    <p><em>Images after 2nd Epoch</em></p>
    <img src="figures/image_at_epoch_0011.png" style="width: 200px;">
    <p><em>Images after 11th Epoch</em></p>
    <img src="figures/image_at_epoch_0101.png" style="width: 200px;">
    <p><em>Images after 100th Epoch</em></p>
</div>



### Experiment with MD-GAN, FL-GAN and IFL-GAN

#### MD-GAN (Multi-Discriminator GAN)
Purpose: Introduces multiple discriminators to enhance training stability and improve the quality of generated data.
Architecture: Instead of a single discriminator, MD-GAN uses multiple discriminators, each learning different aspects or features of the data distribution. 

#### FL-GAN (Federated Learning GAN)
Purpose: Combines GANs with Federated Learning to train models across distributed devices without sharing raw data.
Architecture: Each participant device has its own local GAN (a generator and a discriminator). Model updates are aggregated and sent to a central server, enabling collaborative training while preserving data privacy. The aggregated model parameters are averaged to create the global generator parameters for the next iteration.

#### IFL-GAN (Improved Federated Learning GAN)
Purpose: IFL-GAN builds upon traditional Federated Learning GAN (FL-GAN) concepts by adding a refined mechanism(MMD) for combining and updating local generators across multiple clients.

Architecture: Each participant device has its own local GAN (a generator and a discriminator). After local training, each client computes a Maximum Mean Discrepancy (MMD) score by comparing real data samples against the locally generated samples.These MMD scores capture how well each local generator matches its client’s data distribution. Each client compares its MMD score with a threshold; if the client’s MMD is above the threshold (indicating a poorer fit), it replaces its local generator $𝐺_𝑖$ with the newly formed global generator $𝐺_{glb}$.

According to the results shared in the paper, the hypothesis and experimental structure is decided as follows:
#### Hypotheses

1- Performance Across Different Client Numbers
We hypothesize that increasing the number of clients (i.e., moving from k=2 to k=5 to k=10) in federated learning settings will have varying effects on performance depending on the GAN architecture (MDGAN, FLGAN, or IFLGAN). Specifically, we anticipate that IFLGAN will be more robust than MDGAN and FLGAN as the number of clients grows, owing to its ability to better aggregate gradients and capture global data distributions.

2- Robustness to Data Imbalance
When moving from balanced datasets to imbalanced and extremely imbalanced datasets, we hypothesize that IFLGAN will maintain higher fidelity in generated data and exhibit lower performance degradation than MDGAN and FLGAN. This is because IFLGAN is designed to mitigate divergence caused by heterogeneous data distributions and to reduce the negative effects of highly skewed data partitions across different clients.

3- Improvement Over Centralized Training Baselines
Although centralized GAN training on balanced datasets often yields strong performance, we hypothesize that the federated GAN models—particularly IFLGAN—will demonstrate competitive or superior performance on certain tasks, even in imbalanced scenarios. This arises from the fact that federated training can leverage diverse local data while preserving privacy and preventing overfitting that may occur in a single-site centralized dataset.

4- Impact on Convergence and Stability
Finally, we hypothesize that the federated approach with carefully designed aggregation (IFLGAN) will converge more stably than the simpler distributed methods (MDGAN or FLGAN) as it handles partial updates more effectively. In other words, while MDGAN and FLGAN may converge faster in some cases, IFLGAN will yield more consistent and reliable convergence trends across different levels of data imbalance.

#### Training Procedure
Dataset: MNIST divided into K = 2,5 with partitioning the numbers sequentially (number of clients) parts, normalized to [-1, 1].

Balanced Scenario:

Imbalanced Scenario:

Hyperparameters (for all models): 
- Learning rate = 0.0002
- Batch size = 128
- Adam optimizer with betas = (0.5, 0.999).

## 3.2. Running the code

Each model implementation is included in its .py file with the same name. The models are trained on Google Colab with runtime type as "Python 3" and for faster implementation, the hardware accelerator can be chosen as "T4-GPU". At each 5 epochs, the created images by the global generator are saved to the path "/content/ifl-gan-images", "/content/generated_images", "/content/mdgan_generated_images", respectively.
### Prerequisites
Python 3.7+ (earlier versions might work, but are not tested)
PyTorch >= 1.7 (ensure CUDA is available if you plan to use GPU)
Additional Python packages:
- torchvision
- numpy
- matplotlib
- pandas
- Optional: GPU/CUDA for faster training.
Make sure your environment meets these requirements before installing and running the code.

### Usage
Running the Models
We have three main federated GAN implementations in the repository:
MD-GAN: mdgan.py
FL-GAN: flgan.py
IFL-GAN: iflgan.py
Each script provides an entry point via a main section or a dedicated function for training and testing.

### Configuration Options
Most scripts include parameters you can customize, typically near the bottom of each file or as command-line arguments:

- num_clients (k=2, 5, or 10): Number of federated clients.
- case: Data split scenario: "balanced", "imbalanced", or "extremely_imbalanced".
- epochs: Total training epochs.
- batch_size: Mini-batch size for training.
- lr: Learning rate.
- noise_dim: Dimensionality of the generator’s latent space.
- sigma: (Only in IFL-GAN) Parameter for the MMD kernel.
Adjust these in the source code at the end of the code.

Example:

```
if __name__ == "__main__":
    # Example usage:
    ifl_gan = IFLGAN(
        noise_dim=128, 
        case="balanced",     # or "imbalanced", "extremely_imbalanced"
        num_clients=5,       # 2, 5, or 10
        batch_size=128, 
        lr=0.0002
    )
    ifl_gan.ifl_training(epochs=200, sigma=1.0)
```

## 3.3. Results


### Client K=2, Balanced
   <p align="center">
  <img src="figures/generator_loss_plot_balanced_k=2.png" style="width: 70%;"><br>
  <em>Figure 7: Generator loss, balanced dataset, K=2 clients</em>
</p>
   <p align="center">
     <img src="figures/balanced_k=2.png" style="width: 70%;"><br>
     <em>Figure 8: Generator results (FL-GAN, IFL-GAN, MD-GAN), balanced dataset, K=2 clients</em>
</p>

### Client K=2, Imbalanced
   <p align="center">
  <img src="figures/generator_loss_plot_imbalanced_k=2.png" style="width: 70%;"><br>
  <em>Figure 9: Generator loss, imbalanced dataset, K=2 clients</em>
</p>
<p align="center">
     <img src="figures/imbalanced_k=2.png" style="width: 70%;"><br>
     <em>Figure 10: Generator results (FL-GAN, IFL-GAN, MD-GAN), imbalanced dataset, K=2 clients</em>
</p>

### Client K=5, Balanced
   <p align="center">
  <img src="figures/combined_gan_losses.png=5.png" style="width: 70%;"><br>
  <em>Figure 11: Generator loss, balanced dataset, K=5 clients</em>
</p>
<p align="center">
     <img src="figures/balanced_k=5.png" style="width: 70%;"><br>
     <em>Figure 12: Generator results (FL-GAN, IFL-GAN, MD-GAN), balanced dataset, K=5 clients</em>
</p>

### Client K=5, Imbalanced
   <p align="center">
  <img src="figures/generator_loss_plot_imbalanced_k=5.png" style="width: 70%;"><br>
  <em>Figure 13: Generator loss, imbalanced dataset, K=5 clients</em>
</p>
<p align="center">
     <img src="figures/imbalanced_k=5.png" style="width: 70%;"><br>
     <em>Figure 14: Generator results (FL-GAN, IFL-GAN, MD-GAN), imbalanced dataset, K=5 clients</em>
</p>

## Results from the Paper

<p align="center">
  <img src="figures/paper_generator_loss_k=2.png" style="width: 70%;"><br>
  <em>Figure 15: Generator loss, balanced and imbalanced datasets, K=2 clients</em>
</p>

# 4. Conclusion

In this part, the results shared are discussed from the perspective of the hypotheses created from the paper.

1- Performance Across Different Client Numbers

2- Robustness to Data Imbalance

3- Improvement Over Centralized Training Baselines

4- Impact on Convergence and Stability


# 5. References

@TODO: Provide your references here.

# Contact

email: egemenalacali@gmail.com
