# IFL-GAN: Improved Federated Learning Generative Adversarial Network With Maximum Mean Discrepancy Model Aggregation

This readme file is an outcome of the [CENG501 (Fall 2024)](https://ceng.metu.edu.tr/~skalkan/DL/) project for reproducing a paper without an implementation. See [CENG501 (Fall 2024) Project List](https://github.com/CENG501-Projects/CENG501-Fall2024) for a complete list of all paper reproduction projects.

# 1. Introduction
Generative Adversarial Networks (GANs) are powerful generative models that simulate data distributions by training a generator-discriminator pair in a competitive setting. However, traditional GANs often require centralized and independent, identically distributed (i.i.d.) training data, which is impractical in many real-world scenarios. Data is often distributed among multiple clients and is non-i.i.d., posing challenges for effective GAN training.

Federated Learning (FL) has been applied to address this issue by training GANs in a decentralized manner. Existing approaches, like Federated Learning GAN (FL-GAN), aggregate updates using Federated Averaging (FedAvg), but this method suffers in non-i.i.d. cases, leading to poor convergence and low-quality outputs.

This paper proposes Improved Federated Learning GAN (IFL-GAN), which uses Maximum Mean Discrepancy (MMD) to aggregate updates, assigning weights based on local GAN convergence status. This approach achieves faster training convergence and higher-quality outputs on non-i.i.d. datasets.

## 1.1 Paper Summary
The contributions of the paper can be summarized as:

- A novel framework called IFL-GAN that replaces federating average strategy (FedAvg) with MMD for aggregating local GAN updates, improving performance in non-i.i.d. data settings.
- Theoretical and empirical comparisons of MMD and FedAvg aggregation methods.
- Extensive experimental validation showing superior performance in terms of data diversity and quality across MNIST, CIFAR10, and SVHN datasets.
The IFL-GAN system model is depicted in Figure 1:

<p align="center">
  <img src="figures/system model.png" style="width: 70%;"><br>
  <em>Figure 1: IFL-GAN system model</em>
</p>
 

# 2. Methodology
## 2.1. Key Concepts

### Generative Adversarial Networks
Generative Adversarial Networks (GANs) consist of two neural networks: a Generator (G) and a Discriminator (D), trained in an adversarial manner. The generator creates synthetic data, while the discriminator attempts to distinguish between real and generated data. The GAN objective function is:

$$min_G  max_D V(G, D) = E_{x ~~ p_r(x)} [log D(x)] + E_{z ~~ p_z(z)} [log (1 - D(G(z)))]$$

Where:

$D(x)$: Probability that x is a real data sample.

$G(z)$: Synthetic data generated from input noise z.

$p_r(x)$: Real data distribution.

$p_z(z)$: easy-to-sample Noise distribution (e.g., Gaussian or uniform).

### Federated Learning for GANs
Federated Learning (FL) enables decentralized training across multiple clients without transferring raw data, preserving privacy. Instead, clients train local models and share their updates with a central server. However, training GANs in a federated setting introduces challenges such as:

- Non-i.i.d. Data: Data across clients often have unique distributions.

- Stability Issues: The adversarial nature of GANs makes distributed training more complex.
FL-GAN extends federated learning to GANs by aggregating updates from multiple local GANs to train a global GAN.

### Federated Learning GAN (FL-GAN)
FL-GAN uses a Federated Averaging (FedAvg) strategy to aggregate updates from local GANs into a global GAN model. Each client maintains a local GAN, with a generator (G_i) and a discriminator (D_i).

FL-GAN Objective Function:
The FL-GAN training objective is:

$min_G max_D V(G, D) = (1/K) * Σ_{i=1}^{K} [E_{x ~ p_{r_i}(x)} log D_i(x) + E_{z ~ p_z(z)} log (1 - D_i(G_i(z)))]$

Where:

$K$: Number of clients.

$G_i, D_i$: Local generator and discriminator for client i.

$p_{r_i}(x)$: Real data distribution for client i.

$G_i(z)$: Data generated by the local generator for client i.

### Challenges in FL-GAN
Despite its utility, according to the writers FL-GAN faces several challenges:

Non-i.i.d. Data: FedAvg assumes equal contribution from all clients, which is ineffective when data distributions vary.

Convergence Instability: Aggregating updates from clients with different convergence statuses can destabilize training.

Low Diversity: Uniform weighting may reduce diversity and fidelity in generated data.

These limitations motivate the development of Improved Federated Learning GAN (IFL-GAN), which uses Maximum Mean Discrepancy (MMD) for weighted aggregation to address these issues effectively.

## 2.1.1 IFL-GAN

IFL-GAN introduces an MMD-based mechanism to aggregate local GAN updates, assigning weights dynamically based on each local GAN's convergence status. 

$min_G max_D V(G, D) = (1/K) * Σ_{i=1}^{K} α_i * [E_{x ~ p_{r_i}(x)} log D_i(x) + E_{z ~ p_z(z)} log (1 - D_i(G_i(z)))]$

Unlike traditional Federated Averaging (FedAvg), which equally weights updates ($α_i={1/K}$ for all i), this method ensures that poorly converged models contribute more to the global model, leading to better stability and quality of generated data in non-i.i.d. settings. The pseudocode for the algorithm is shared in Figure 2.

<p align="center">
  <img src="figures/pseudocode for the algorithm.png" style="width: 70%;"><br>
  <em>Figure 2: IFL-GAN Algorithm</em>
</p>

## 2.1.2. IFL-GAN Algorithm Steps

Step 1: Initialization
A global GAN model is initialized, consisting of a generator $G_{global}$, and a discriminator $D_{global}$. The global model parameters are distributed to all $K$ clients.

Step 2: Local Training
Each client trains its local GAN on its private, potentially non-i.i.d. dataset: 

1. For client $i$, a local generator $G_i$ and a discriminator $D_i$ are initialized with the global model parameters.
   
2. The local GAN is trained using the standard GAN objective function:
   
3. After a fixed number of local training epochs, the client sends the updated parameters $\theta_{G_i}$ (from the generator) and $\theta_{D_i}$(from the discriminator) to the central server.

Step 3: MMD-Based Model Aggregation
The server aggregates the local model updates using Maximum Mean Discrepancy (MMD):

1. Compute the MMD score for each client's local generator: $MMD_i = MMD(p_{r_{\text{global}}}, p_{r_i})$
2. Convert MMD scores into aggregation weights: $α_i = exp(-MMD_i) / Σ_{j=1}^{K} exp(-MMD_j)$
3. Update the global generator parameters: $θ_{G_{\text{global}}} = Σ_{i=1}^{K} α_i * θ_{G_i}$

Step 4: Discriminator Aggregation
For the discriminator, the server aggregates updates similarly, though discriminator-specific adjustments may apply.

Step 5: Synchronization
The updated global generator $G_{global}$ and discriminator $D_{global}$ are sent back to all clients, replacing their local models:
$G_i = G_{\text{global}}, D_i = D_{\text{global}}, for i in [1, K]$

## 2.2. Our interpretation

  The overall aim of the work is represented thoroughly in the paper. However, the basic model that is tried to be improved upon, namely the FL-GAN is not discussed algorithm-wise. So it has been decided that to create a better understanding of the idea a more modular approach should be followed. In this part, the nuances will be discussed and in the experimental setup part (Part 3), the modularity will be shared.

1. The Discussion of the Method of FL-GAN
  As discussed in the previous chapters, the aim of studying GANs using the framework of federated learning is to fully utilize the distributed data across different devices for learning purposes. Thus, the results obtained from FL-GAN are used as a benchmark for the performance comparison of IFL-GAN in the paper. As this is going to be the first GAN and Federated Learning implementation of the author of this repository, it is decided that the GAN structure should be studied briefly. After that, the iid local dataset division, client model aggregation and global parameter distribution mechanisms are decided to be introduced to the basic model for FL-GAN application.

2. Shared Architectural Details
   In figure 3, the architectural details used in the study are shared. In detail, the IFL-GAN framework has been tried on three different datasets: MNIST, CIFAR10, and SVHN datasets. These are divided into non-iid groups to simulate local datasets of the users. However, according to the supposed data input dimensions, the architectures shared in figure 3 are not consistent. For instance, a data from MNIST dataset is a 28x28 grayscale image but the discriminator architecture is started with a 1x28 fully connected layer. So, it is decided that to get ahold of the GAN model first, a proven GAN structure is going to be used until the original idea is understood. Also, in [2], deep convolutional networks are introduced such that fully connected hidden layers are removed for deeper architectures. Such models can be trained on edge devices for better performance as well.

   <p align="center">
  <img src="figures/Architectural Details.png" style="width: 70%;"><br>
  <em>Figure 3: Architectural Details of IFL-GAN </em>
</p>


# 3. Experiments and results

## 3.1. Experimental setup

As shared in part 2.2, it is decided that since the IFL-GAN consists of a number of locally trained GANs, firstly some variations of different GAN networks are studied. 
### First Experiment with Central GAN

#### Objective and Architecture
Generator: Converts random noise (NOISE_DIM=100) into realistic 28x28 images using layers of linear transformations, reshaping, transposed convolutions, batch normalization, ReLU, and Tanh activations.

Discriminator: Distinguishes real images from fake ones using convolutional layers, batch normalization, and LeakyReLU activation, outputting a single probability value.

#### Experimental Setup
Dataset: MNIST, normalized to [-1, 1].
Hyperparameters: Learning rate = 0.0002, batch size = 256, Adam optimizer with betas = (0.5, 0.999).

#### Training Procedure
- Train Discriminator:

Calculate loss (real_loss) for real images.
Calculate loss (fake_loss) for fake images.
Minimize combined loss (real_loss + fake_loss).

- Train Generator:

Generate fake images and calculate loss (gen_loss) based on discriminator output.
Update the generator to maximize the discriminator’s error on fake images.

#### Hypothesis
Loss Trends:
Discriminator loss is expected to start high relatively, then decreases as it improves.
Generator loss decreases as generated images become realistic.
In the ideal case, both losses should converge to 50%.


![](https://github.com/CENG501_Fall2024/alacali/figures/central_gan_1st_try.gif)



### Second Experiment with IID FL-GAN


## 3.2. Running the code

@TODO: Explain your code & directory structure and how other people can run it.

## 3.3. Results

@TODO: Present your results and compare them to the original paper. Please number your figures & tables as if this is a paper.

# 4. Conclusion

@TODO: Discuss the paper in relation to the results in the paper and your results.

# 5. References

@TODO: Provide your references here.

# Contact

@TODO: Provide your names & email addresses and any other info with which people can contact you.
